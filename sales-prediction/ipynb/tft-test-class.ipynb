{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15d5b5b-f6a7-4112-887c-3f3a896e7ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import RMSE\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class TFTModel:\n",
    "    def __init__(self, data_path, features, target_column, max_encoder_length, max_prediction_length, batch_size, checkpoint_dir=\"./checkpoints\", model_save_dir=\"./weights\"):\n",
    "        # 데이터 로드 및 전처리\n",
    "        self.df = pd.read_csv(data_path)\n",
    "        self.features = features\n",
    "        self.target_column = target_column\n",
    "        self.max_encoder_length = max_encoder_length\n",
    "        self.max_prediction_length = max_prediction_length\n",
    "        self.batch_size = batch_size\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.model_save_dir = model_save_dir\n",
    "\n",
    "        self.df['product_id'] = self.df['product_id'].astype(str)\n",
    "        self.df['week_date'] = pd.to_datetime(self.df['week_date'])\n",
    "        self.df = self.df.sort_values(\"week_date\").reset_index(drop=True)\n",
    "        self.df[\"time_idx\"] = (self.df[\"week_date\"] - self.df[\"week_date\"].min()).dt.days // 7  # 주 단위로 인덱스 생성\n",
    "\n",
    "        # 8:2로 train과 validation 분리\n",
    "        self.train_df, self.val_df = train_test_split(self.df, test_size=0.2, shuffle=False)\n",
    "        \n",
    "        # 훈련 데이터 준비\n",
    "        self.training_cutoff = self.train_df['time_idx'].max() - max_prediction_length\n",
    "        self.training = TimeSeriesDataSet(\n",
    "            self.train_df[lambda x: x.time_idx <= self.training_cutoff],\n",
    "            time_idx=\"time_idx\",\n",
    "            target=self.target_column,\n",
    "            group_ids=[\"product_id\"],\n",
    "            min_encoder_length=max_encoder_length // 2,\n",
    "            max_encoder_length=max_encoder_length,\n",
    "            max_prediction_length=max_prediction_length,\n",
    "            static_categoricals=[\"product_id\"],\n",
    "            time_varying_known_reals=[\"time_idx\"] + features,\n",
    "            time_varying_unknown_reals=[target_column],\n",
    "            target_normalizer=GroupNormalizer(groups=[\"product_id\"]),\n",
    "        )\n",
    "\n",
    "        # 모델 정의\n",
    "        self.tft = TemporalFusionTransformer.from_dataset(\n",
    "            self.training,\n",
    "            learning_rate=0.03,\n",
    "            hidden_size=16,\n",
    "            attention_head_size=1,\n",
    "            dropout=0.1,\n",
    "            hidden_continuous_size=8,\n",
    "            output_size=1,\n",
    "            loss=RMSE(),\n",
    "        )\n",
    "\n",
    "        # Trainer 설정\n",
    "        self.trainer = pl.Trainer(\n",
    "            max_epochs=100,\n",
    "            devices=1,\n",
    "            accelerator='gpu',\n",
    "            gradient_clip_val=0.1,\n",
    "            logger=pl.loggers.TensorBoardLogger('tb_logs'),\n",
    "            callbacks=[pl.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)]\n",
    "        )\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        학습을 진행하는 함수\n",
    "        \"\"\"\n",
    "        # 데이터 로더 준비\n",
    "        train_dataloader = self.training.to_dataloader(train=True, batch_size=self.batch_size, num_workers=0)\n",
    "\n",
    "        # 모델 학습\n",
    "        pl.seed_everything(42)\n",
    "        self.trainer.fit(self.tft, train_dataloaders=train_dataloader)\n",
    "\n",
    "        # 체크포인트 저장\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "        checkpoint_path = os.path.join(self.checkpoint_dir, \"tft_model_checkpoint.ckpt\")\n",
    "        self.trainer.save_checkpoint(checkpoint_path)\n",
    "\n",
    "        # 모델 가중치 저장\n",
    "        os.makedirs(self.model_save_dir, exist_ok=True)\n",
    "        model_save_path = os.path.join(self.model_save_dir, \"tft_model_weights.pth\")\n",
    "        torch.save(self.tft.state_dict(), model_save_path)\n",
    "\n",
    "        print(\"모델 학습 완료 및 저장.\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        validation 데이터를 사용하여 모델 성능 평가\n",
    "        \"\"\"\n",
    "        val_dataloader = self.training.to_dataloader(train=False, batch_size=self.batch_size, num_workers=0)\n",
    "        predictions = self.trainer.predict(self.tft, val_dataloader)\n",
    "\n",
    "        # 실제 값과 예측값 비교\n",
    "        true_values = self.val_df[self.target_column].values\n",
    "        mse = mean_squared_error(true_values, predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(true_values, predictions)\n",
    "        \n",
    "        print(f\"Validation RMSE: {rmse:.4f}, R2: {r2:.4f}\")\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def plot_predictions(self, product_ids):\n",
    "        \"\"\"\n",
    "        각 상품별 마지막 주 예측 결과를 실제 값과 함께 시각화\n",
    "        \"\"\"\n",
    "        for product_id in product_ids:\n",
    "            product_data = self.val_df[self.val_df['product_id'] == product_id]\n",
    "            product_data = product_data.sort_values(\"week_date\")\n",
    "\n",
    "            # 실제값 (파란색)과 예측값 (빨간색)\n",
    "            true_values = product_data[self.target_column].values\n",
    "            last_week_data = product_data.tail(1)\n",
    "            prediction = self.trainer.predict(self.tft, self.training.to_dataloader(train=False, batch_size=self.batch_size, num_workers=0))\n",
    "\n",
    "            # 그래프 그리기\n",
    "            print(product_data['product_name'])\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(product_data['week_date'], true_values, color='blue', label='Actual')\n",
    "            plt.scatter(last_week_data['week_date'], prediction, color='red', label='Prediction', zorder=5)\n",
    "            plt.title(f\"Product {product_id}: Actual vs Predicted for Last Week\")\n",
    "            plt.xlabel(\"Week\")\n",
    "            plt.ylabel(\"Sales\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "    def predict_next_week(self, product_id):\n",
    "        \"\"\"\n",
    "        마지막 주에 대해 다음 주 예측값을 계산하고 비교\n",
    "        \"\"\"\n",
    "        product_data = self.df[self.df['product_id'] == product_id]\n",
    "        last_week_sales = product_data['week_purchase_cnt'].iloc[-1]\n",
    "\n",
    "        # 예측값 (다음 주 판매량)\n",
    "        next_week_prediction = self.trainer.predict(self.tft, self.training.to_dataloader(train=False, batch_size=self.batch_size, num_workers=0))[-1]\n",
    "\n",
    "        # 비교: 평균, 최대, 최소, 마지막 주 판매량\n",
    "        mean_weekly_sales = product_data['week_purchase_cnt'].mean()\n",
    "        max_weekly_sales = product_data['week_purchase_cnt'].max()\n",
    "        min_weekly_sales = product_data['week_purchase_cnt'].min()\n",
    "\n",
    "        print(f\"Product {product_id}:\")\n",
    "        print(f\"  Mean Weekly Sales: {mean_weekly_sales:.2f}\")\n",
    "        print(f\"  Max Weekly Sales: {max_weekly_sales:.2f}\")\n",
    "        print(f\"  Min Weekly Sales: {min_weekly_sales:.2f}\")\n",
    "        print(f\"  Last Week Sales: {last_week_sales:.2f}\")\n",
    "        print(f\"  Predicted Next Week Sales: {next_week_prediction:.2f}\")\n",
    "        \n",
    "        return next_week_prediction\n",
    "\n",
    "# 사용 예시\n",
    "#if __name__ == \"__main__\":\n",
    "# 필요한 변수 설정\n",
    "data_path = \"./fina_preprocessing_data.csv\"\n",
    "features = ['price', 'review_cnt', 'wish_cnt', 'sixMothRatio(puchase_cnt/review_cnt)', 'week_review_count', 'average_review_score', 'category1_encoded',\n",
    "           'category2_encoded', 'category3_encoded', 'rolling_mean_purchase', 'rolling_std_purchase', 'week_num', 'month', 'month_sin', 'month_cos', \n",
    "            'week_sin', 'week_cos']\n",
    "\n",
    "target_column = 'week_purchase_cnt'\n",
    "max_encoder_length = 24\n",
    "max_prediction_length = 1\n",
    "batch_size = 64\n",
    "\n",
    "# 모델 인스턴스 생성 및 학습 진행\n",
    "tft_model = TFTModel(data_path, features, target_column, max_encoder_length, max_prediction_length, batch_size)\n",
    "tft_model.fit()\n",
    "\n",
    "# validation 데이터에 대한 예측\n",
    "predictions = tft_model.evaluate()\n",
    "\n",
    "# 예시 3개의 상품에 대해 마지막 주 예측 결과를 시각화\n",
    "random_product_ids = df['product_id'].sample(n=5, random_state=42).tolist()\n",
    "tft_model.plot_predictions(random_product_ids)  # 예시로 1, 2, 3번 상품에 대해 시각화\n",
    "\n",
    "# 특정 상품에 대해 다음 주 예측값을 계산 및 비교\n",
    "product_id = 1\n",
    "tft_model.predict_next_week(product_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6585b538-75a3-49cf-84d0-9bae09f8ac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "week_date                               datetime64[ns]\n",
    "product_id                                       int64\n",
    "product_name                                    object\n",
    "price                                          float64\n",
    "review_cnt                                     float64\n",
    "purchase_cnt                                   float64\n",
    "wish_cnt                                       float64\n",
    "sixMothRatio(puchase_cnt/review_cnt)           float64\n",
    "week_review_count                              float64\n",
    "average_review_score                           float64\n",
    "week_purchase_cnt                              float64\n",
    "category1_encoded                              float64\n",
    "category2_encoded                              float64\n",
    "category3_encoded                              float64\n",
    "rolling_mean_purchase                          float64\n",
    "rolling_std_purchase                           float64\n",
    "week_num                                        UInt32\n",
    "month                                            int32\n",
    "month_sin                                      float64\n",
    "month_cos                                      float64\n",
    "week_sin                                       Float64\n",
    "week_cos                                       Float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34d93a08-027d-4d8a-9634-4d38d2a3e28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import RMSE\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db5705d6-1fd2-4414-8dc9-e9db7e5d060f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week_date</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>price</th>\n",
       "      <th>review_cnt</th>\n",
       "      <th>purchase_cnt</th>\n",
       "      <th>wish_cnt</th>\n",
       "      <th>sixMothRatio(puchase_cnt/review_cnt)</th>\n",
       "      <th>week_review_count</th>\n",
       "      <th>average_review_score</th>\n",
       "      <th>...</th>\n",
       "      <th>category2_encoded</th>\n",
       "      <th>category3_encoded</th>\n",
       "      <th>rolling_mean_purchase</th>\n",
       "      <th>rolling_std_purchase</th>\n",
       "      <th>week_num</th>\n",
       "      <th>month</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>week_sin</th>\n",
       "      <th>week_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-25</td>\n",
       "      <td>6356018199</td>\n",
       "      <td>DIY 목재재단 나무 원목 합판 집성목 MDF 방부목 자작나무 히노끼</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>61440.0</td>\n",
       "      <td>3745.0</td>\n",
       "      <td>1685.0</td>\n",
       "      <td>2.386871</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>68.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>14.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-10-02</td>\n",
       "      <td>6356018199</td>\n",
       "      <td>DIY 목재재단 나무 원목 합판 집성목 MDF 방부목 자작나무 히노끼</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>61440.0</td>\n",
       "      <td>3745.0</td>\n",
       "      <td>1685.0</td>\n",
       "      <td>2.386871</td>\n",
       "      <td>71.0</td>\n",
       "      <td>4.873239</td>\n",
       "      <td>...</td>\n",
       "      <td>68.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>91.50</td>\n",
       "      <td>109.601551</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-0.992709</td>\n",
       "      <td>1.205367e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-10-09</td>\n",
       "      <td>6356018199</td>\n",
       "      <td>DIY 목재재단 나무 원목 합판 집성목 MDF 방부목 자작나무 히노끼</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>61440.0</td>\n",
       "      <td>3745.0</td>\n",
       "      <td>1685.0</td>\n",
       "      <td>2.386871</td>\n",
       "      <td>97.0</td>\n",
       "      <td>4.742268</td>\n",
       "      <td>...</td>\n",
       "      <td>68.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>138.00</td>\n",
       "      <td>111.772090</td>\n",
       "      <td>41</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-0.970942</td>\n",
       "      <td>2.393157e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-10-16</td>\n",
       "      <td>6356018199</td>\n",
       "      <td>DIY 목재재단 나무 원목 합판 집성목 MDF 방부목 자작나무 히노끼</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>61440.0</td>\n",
       "      <td>3745.0</td>\n",
       "      <td>1685.0</td>\n",
       "      <td>2.386871</td>\n",
       "      <td>108.0</td>\n",
       "      <td>4.740741</td>\n",
       "      <td>...</td>\n",
       "      <td>68.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>167.75</td>\n",
       "      <td>108.944558</td>\n",
       "      <td>42</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-0.935016</td>\n",
       "      <td>3.546049e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-10-23</td>\n",
       "      <td>6356018199</td>\n",
       "      <td>DIY 목재재단 나무 원목 합판 집성목 MDF 방부목 자작나무 히노끼</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>61440.0</td>\n",
       "      <td>3745.0</td>\n",
       "      <td>1685.0</td>\n",
       "      <td>2.386871</td>\n",
       "      <td>89.0</td>\n",
       "      <td>4.853933</td>\n",
       "      <td>...</td>\n",
       "      <td>68.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>176.60</td>\n",
       "      <td>96.401763</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-0.885456</td>\n",
       "      <td>4.647232e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23211</th>\n",
       "      <td>2024-09-30</td>\n",
       "      <td>88459191933</td>\n",
       "      <td>모리모토 뉴스타 M1 내야 외야 투수 올라운드 야구 글러브 우투 좌투</td>\n",
       "      <td>79000.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.777778</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>23.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>-0.992709</td>\n",
       "      <td>1.205367e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23212</th>\n",
       "      <td>2024-10-07</td>\n",
       "      <td>88459191933</td>\n",
       "      <td>모리모토 뉴스타 M1 내야 외야 투수 올라운드 야구 글러브 우투 좌투</td>\n",
       "      <td>79000.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.777778</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>25.50</td>\n",
       "      <td>3.535534</td>\n",
       "      <td>41</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-0.970942</td>\n",
       "      <td>2.393157e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23213</th>\n",
       "      <td>2024-09-30</td>\n",
       "      <td>88473568229</td>\n",
       "      <td>노다지 영양 양곰탕 750g 7팩 곱창전골 내장탕 한우사골 즉석국 간편식</td>\n",
       "      <td>49900.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1088.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>8.845528</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>-0.992709</td>\n",
       "      <td>1.205367e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23214</th>\n",
       "      <td>2024-10-07</td>\n",
       "      <td>88473568229</td>\n",
       "      <td>노다지 영양 양곰탕 750g 7팩 곱창전골 내장탕 한우사골 즉석국 간편식</td>\n",
       "      <td>49900.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1088.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>8.845528</td>\n",
       "      <td>93.0</td>\n",
       "      <td>4.526882</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>415.00</td>\n",
       "      <td>575.584920</td>\n",
       "      <td>41</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-0.970942</td>\n",
       "      <td>2.393157e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23215</th>\n",
       "      <td>2024-10-14</td>\n",
       "      <td>88473568229</td>\n",
       "      <td>노다지 영양 양곰탕 750g 7팩 곱창전골 내장탕 한우사골 즉석국 간편식</td>\n",
       "      <td>49900.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1088.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>8.845528</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4.551724</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>362.00</td>\n",
       "      <td>417.224160</td>\n",
       "      <td>42</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-0.935016</td>\n",
       "      <td>3.546049e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23216 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        week_date   product_id                              product_name  \\\n",
       "0      2023-09-25   6356018199    DIY 목재재단 나무 원목 합판 집성목 MDF 방부목 자작나무 히노끼   \n",
       "1      2023-10-02   6356018199    DIY 목재재단 나무 원목 합판 집성목 MDF 방부목 자작나무 히노끼   \n",
       "2      2023-10-09   6356018199    DIY 목재재단 나무 원목 합판 집성목 MDF 방부목 자작나무 히노끼   \n",
       "3      2023-10-16   6356018199    DIY 목재재단 나무 원목 합판 집성목 MDF 방부목 자작나무 히노끼   \n",
       "4      2023-10-23   6356018199    DIY 목재재단 나무 원목 합판 집성목 MDF 방부목 자작나무 히노끼   \n",
       "...           ...          ...                                       ...   \n",
       "23211  2024-09-30  88459191933    모리모토 뉴스타 M1 내야 외야 투수 올라운드 야구 글러브 우투 좌투   \n",
       "23212  2024-10-07  88459191933    모리모토 뉴스타 M1 내야 외야 투수 올라운드 야구 글러브 우투 좌투   \n",
       "23213  2024-09-30  88473568229  노다지 영양 양곰탕 750g 7팩 곱창전골 내장탕 한우사골 즉석국 간편식   \n",
       "23214  2024-10-07  88473568229  노다지 영양 양곰탕 750g 7팩 곱창전골 내장탕 한우사골 즉석국 간편식   \n",
       "23215  2024-10-14  88473568229  노다지 영양 양곰탕 750g 7팩 곱창전골 내장탕 한우사골 즉석국 간편식   \n",
       "\n",
       "         price  review_cnt  purchase_cnt  wish_cnt  \\\n",
       "0       1000.0     61440.0        3745.0    1685.0   \n",
       "1       1000.0     61440.0        3745.0    1685.0   \n",
       "2       1000.0     61440.0        3745.0    1685.0   \n",
       "3       1000.0     61440.0        3745.0    1685.0   \n",
       "4       1000.0     61440.0        3745.0    1685.0   \n",
       "...        ...         ...           ...       ...   \n",
       "23211  79000.0         8.0          52.0      25.0   \n",
       "23212  79000.0         8.0          52.0      25.0   \n",
       "23213  49900.0        70.0        1088.0     101.0   \n",
       "23214  49900.0        70.0        1088.0     101.0   \n",
       "23215  49900.0        70.0        1088.0     101.0   \n",
       "\n",
       "       sixMothRatio(puchase_cnt/review_cnt)  week_review_count  \\\n",
       "0                                  2.386871                6.0   \n",
       "1                                  2.386871               71.0   \n",
       "2                                  2.386871               97.0   \n",
       "3                                  2.386871              108.0   \n",
       "4                                  2.386871               89.0   \n",
       "...                                     ...                ...   \n",
       "23211                              5.777778                4.0   \n",
       "23212                              5.777778                5.0   \n",
       "23213                              8.845528                1.0   \n",
       "23214                              8.845528               93.0   \n",
       "23215                              8.845528               29.0   \n",
       "\n",
       "       average_review_score  ...  category2_encoded  category3_encoded  \\\n",
       "0                  4.666667  ...               68.0               99.0   \n",
       "1                  4.873239  ...               68.0               99.0   \n",
       "2                  4.742268  ...               68.0               99.0   \n",
       "3                  4.740741  ...               68.0               99.0   \n",
       "4                  4.853933  ...               68.0               99.0   \n",
       "...                     ...  ...                ...                ...   \n",
       "23211              4.750000  ...               13.0              184.0   \n",
       "23212              4.800000  ...               13.0              184.0   \n",
       "23213              3.000000  ...                5.0              202.0   \n",
       "23214              4.526882  ...                5.0              202.0   \n",
       "23215              4.551724  ...                5.0              202.0   \n",
       "\n",
       "       rolling_mean_purchase  rolling_std_purchase  week_num  month  \\\n",
       "0                      14.00              0.000000        39      9   \n",
       "1                      91.50            109.601551        40     10   \n",
       "2                     138.00            111.772090        41     10   \n",
       "3                     167.75            108.944558        42     10   \n",
       "4                     176.60             96.401763        43     10   \n",
       "...                      ...                   ...       ...    ...   \n",
       "23211                  23.00              0.000000        40      9   \n",
       "23212                  25.50              3.535534        41     10   \n",
       "23213                   8.00              0.000000        40      9   \n",
       "23214                 415.00            575.584920        41     10   \n",
       "23215                 362.00            417.224160        42     10   \n",
       "\n",
       "       month_sin     month_cos  week_sin      week_cos  \n",
       "0      -1.000000 -1.836970e-16 -1.000000 -1.836970e-16  \n",
       "1      -0.866025  5.000000e-01 -0.992709  1.205367e-01  \n",
       "2      -0.866025  5.000000e-01 -0.970942  2.393157e-01  \n",
       "3      -0.866025  5.000000e-01 -0.935016  3.546049e-01  \n",
       "4      -0.866025  5.000000e-01 -0.885456  4.647232e-01  \n",
       "...          ...           ...       ...           ...  \n",
       "23211  -1.000000 -1.836970e-16 -0.992709  1.205367e-01  \n",
       "23212  -0.866025  5.000000e-01 -0.970942  2.393157e-01  \n",
       "23213  -1.000000 -1.836970e-16 -0.992709  1.205367e-01  \n",
       "23214  -0.866025  5.000000e-01 -0.970942  2.393157e-01  \n",
       "23215  -0.866025  5.000000e-01 -0.935016  3.546049e-01  \n",
       "\n",
       "[23216 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"./fina_preprocessing_data.csv\"\n",
    "ds = pd.read_csv(data_path)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a632c586-749a-4dbb-b3c9-c830b548cd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week_date                                object\n",
      "product_id                                int64\n",
      "product_name                             object\n",
      "price                                   float64\n",
      "review_cnt                              float64\n",
      "purchase_cnt                            float64\n",
      "wish_cnt                                float64\n",
      "sixMothRatio(puchase_cnt/review_cnt)    float64\n",
      "week_review_count                       float64\n",
      "average_review_score                    float64\n",
      "week_purchase_cnt                       float64\n",
      "category1_encoded                       float64\n",
      "category2_encoded                       float64\n",
      "category3_encoded                       float64\n",
      "rolling_mean_purchase                   float64\n",
      "rolling_std_purchase                    float64\n",
      "week_num                                  int64\n",
      "month                                     int64\n",
      "month_sin                               float64\n",
      "month_cos                               float64\n",
      "week_sin                                float64\n",
      "week_cos                                float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(ds.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19ba111b-9162-48b9-aa6c-3c9a95c82fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week_date</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>price</th>\n",
       "      <th>review_cnt</th>\n",
       "      <th>purchase_cnt</th>\n",
       "      <th>wish_cnt</th>\n",
       "      <th>sixMothRatio(puchase_cnt/review_cnt)</th>\n",
       "      <th>week_review_count</th>\n",
       "      <th>average_review_score</th>\n",
       "      <th>...</th>\n",
       "      <th>category2_encoded</th>\n",
       "      <th>category3_encoded</th>\n",
       "      <th>rolling_mean_purchase</th>\n",
       "      <th>rolling_std_purchase</th>\n",
       "      <th>week_num</th>\n",
       "      <th>month</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>week_sin</th>\n",
       "      <th>week_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23047</th>\n",
       "      <td>2024-08-05</td>\n",
       "      <td>88208430684</td>\n",
       "      <td>[9/3블랙예약발송]칼로 네오디뮴 스트랩 3600 활력 부스터 에너지 팔찌</td>\n",
       "      <td>109000.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>567.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>3.910345</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>39.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>-0.663123</td>\n",
       "      <td>-7.485107e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23048</th>\n",
       "      <td>2024-08-12</td>\n",
       "      <td>88208430684</td>\n",
       "      <td>[9/3블랙예약발송]칼로 네오디뮴 스트랩 3600 활력 부스터 에너지 팔찌</td>\n",
       "      <td>109000.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>567.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>3.910345</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.941176</td>\n",
       "      <td>...</td>\n",
       "      <td>39.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>50.500000</td>\n",
       "      <td>21.920310</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>-0.748511</td>\n",
       "      <td>-6.631227e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23049</th>\n",
       "      <td>2024-08-19</td>\n",
       "      <td>88208430684</td>\n",
       "      <td>[9/3블랙예약발송]칼로 네오디뮴 스트랩 3600 활력 부스터 에너지 팔찌</td>\n",
       "      <td>109000.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>567.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>3.910345</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>39.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>45.333333</td>\n",
       "      <td>17.897858</td>\n",
       "      <td>34</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>-0.822984</td>\n",
       "      <td>-5.680647e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23050</th>\n",
       "      <td>2024-08-26</td>\n",
       "      <td>88208430684</td>\n",
       "      <td>[9/3블랙예약발송]칼로 네오디뮴 스트랩 3600 활력 부스터 에너지 팔찌</td>\n",
       "      <td>109000.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>567.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>3.910345</td>\n",
       "      <td>24.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>39.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>57.250000</td>\n",
       "      <td>27.956812</td>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>-0.885456</td>\n",
       "      <td>-4.647232e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23051</th>\n",
       "      <td>2024-09-02</td>\n",
       "      <td>88208430684</td>\n",
       "      <td>[9/3블랙예약발송]칼로 네오디뮴 스트랩 3600 활력 부스터 에너지 팔찌</td>\n",
       "      <td>109000.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>567.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>3.910345</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>39.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>46.400000</td>\n",
       "      <td>34.275356</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>-0.935016</td>\n",
       "      <td>-3.546049e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23052</th>\n",
       "      <td>2024-09-09</td>\n",
       "      <td>88208430684</td>\n",
       "      <td>(팔찌 사전예약)칼로 네오디뮴 스트랩 3600 활력 부스터 에너지 팔찌/목걸이</td>\n",
       "      <td>109000.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>567.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>3.910345</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.777778</td>\n",
       "      <td>...</td>\n",
       "      <td>39.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>44.500000</td>\n",
       "      <td>31.008063</td>\n",
       "      <td>37</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>-0.970942</td>\n",
       "      <td>-2.393157e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23053</th>\n",
       "      <td>2024-09-16</td>\n",
       "      <td>88208430684</td>\n",
       "      <td>[9/3블랙예약발송]칼로 네오디뮴 스트랩 3600 활력 부스터 에너지 팔찌</td>\n",
       "      <td>109000.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>567.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>3.910345</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.454545</td>\n",
       "      <td>...</td>\n",
       "      <td>39.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>45.833333</td>\n",
       "      <td>30.688217</td>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>-0.992709</td>\n",
       "      <td>-1.205367e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23054</th>\n",
       "      <td>2024-09-23</td>\n",
       "      <td>88208430684</td>\n",
       "      <td>칼로 네오디뮴 스트랩 3600 활력 부스터 에너지 팔찌/목걸이</td>\n",
       "      <td>109000.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>567.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>3.910345</td>\n",
       "      <td>26.0</td>\n",
       "      <td>4.653846</td>\n",
       "      <td>...</td>\n",
       "      <td>39.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>51.666667</td>\n",
       "      <td>37.792415</td>\n",
       "      <td>39</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23055</th>\n",
       "      <td>2024-09-30</td>\n",
       "      <td>88208430684</td>\n",
       "      <td>칼로 네오디뮴 스트랩 3600 활력 부스터 에너지 팔찌/목걸이</td>\n",
       "      <td>109000.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>567.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>3.910345</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.454545</td>\n",
       "      <td>...</td>\n",
       "      <td>39.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>60.166667</td>\n",
       "      <td>39.009828</td>\n",
       "      <td>40</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>-0.992709</td>\n",
       "      <td>1.205367e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23056</th>\n",
       "      <td>2024-10-07</td>\n",
       "      <td>88208430684</td>\n",
       "      <td>칼로 네오디뮴 스트랩 3600 활력 부스터 에너지 팔찌/목걸이</td>\n",
       "      <td>109000.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>567.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>3.910345</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.294118</td>\n",
       "      <td>...</td>\n",
       "      <td>39.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>55.666667</td>\n",
       "      <td>35.898004</td>\n",
       "      <td>41</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-0.970942</td>\n",
       "      <td>2.393157e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        week_date   product_id                                 product_name  \\\n",
       "23047  2024-08-05  88208430684    [9/3블랙예약발송]칼로 네오디뮴 스트랩 3600 활력 부스터 에너지 팔찌   \n",
       "23048  2024-08-12  88208430684    [9/3블랙예약발송]칼로 네오디뮴 스트랩 3600 활력 부스터 에너지 팔찌   \n",
       "23049  2024-08-19  88208430684    [9/3블랙예약발송]칼로 네오디뮴 스트랩 3600 활력 부스터 에너지 팔찌   \n",
       "23050  2024-08-26  88208430684    [9/3블랙예약발송]칼로 네오디뮴 스트랩 3600 활력 부스터 에너지 팔찌   \n",
       "23051  2024-09-02  88208430684    [9/3블랙예약발송]칼로 네오디뮴 스트랩 3600 활력 부스터 에너지 팔찌   \n",
       "23052  2024-09-09  88208430684  (팔찌 사전예약)칼로 네오디뮴 스트랩 3600 활력 부스터 에너지 팔찌/목걸이   \n",
       "23053  2024-09-16  88208430684    [9/3블랙예약발송]칼로 네오디뮴 스트랩 3600 활력 부스터 에너지 팔찌   \n",
       "23054  2024-09-23  88208430684           칼로 네오디뮴 스트랩 3600 활력 부스터 에너지 팔찌/목걸이   \n",
       "23055  2024-09-30  88208430684           칼로 네오디뮴 스트랩 3600 활력 부스터 에너지 팔찌/목걸이   \n",
       "23056  2024-10-07  88208430684           칼로 네오디뮴 스트랩 3600 활력 부스터 에너지 팔찌/목걸이   \n",
       "\n",
       "          price  review_cnt  purchase_cnt  wish_cnt  \\\n",
       "23047  109000.0       142.0         567.0     151.0   \n",
       "23048  109000.0       142.0         567.0     151.0   \n",
       "23049  109000.0       142.0         567.0     151.0   \n",
       "23050  109000.0       142.0         567.0     151.0   \n",
       "23051  109000.0       142.0         567.0     151.0   \n",
       "23052  109000.0       142.0         567.0     151.0   \n",
       "23053  109000.0       142.0         567.0     151.0   \n",
       "23054  109000.0       142.0         567.0     151.0   \n",
       "23055  109000.0       142.0         567.0     151.0   \n",
       "23056  109000.0       142.0         567.0     151.0   \n",
       "\n",
       "       sixMothRatio(puchase_cnt/review_cnt)  week_review_count  \\\n",
       "23047                              3.910345                9.0   \n",
       "23048                              3.910345               17.0   \n",
       "23049                              3.910345                9.0   \n",
       "23050                              3.910345               24.0   \n",
       "23051                              3.910345                1.0   \n",
       "23052                              3.910345                9.0   \n",
       "23053                              3.910345               11.0   \n",
       "23054                              3.910345               26.0   \n",
       "23055                              3.910345               22.0   \n",
       "23056                              3.910345               17.0   \n",
       "\n",
       "       average_review_score  ...  category2_encoded  category3_encoded  \\\n",
       "23047              5.000000  ...               39.0              219.0   \n",
       "23048              4.941176  ...               39.0              219.0   \n",
       "23049              5.000000  ...               39.0              219.0   \n",
       "23050              5.000000  ...               39.0              219.0   \n",
       "23051              5.000000  ...               39.0              219.0   \n",
       "23052              4.777778  ...               39.0              219.0   \n",
       "23053              4.454545  ...               39.0              219.0   \n",
       "23054              4.653846  ...               39.0              219.0   \n",
       "23055              4.454545  ...               39.0              219.0   \n",
       "23056              4.294118  ...               39.0              219.0   \n",
       "\n",
       "       rolling_mean_purchase  rolling_std_purchase  week_num  month  \\\n",
       "23047              35.000000              0.000000        32      8   \n",
       "23048              50.500000             21.920310        33      8   \n",
       "23049              45.333333             17.897858        34      8   \n",
       "23050              57.250000             27.956812        35      8   \n",
       "23051              46.400000             34.275356        36      9   \n",
       "23052              44.500000             31.008063        37      9   \n",
       "23053              45.833333             30.688217        38      9   \n",
       "23054              51.666667             37.792415        39      9   \n",
       "23055              60.166667             39.009828        40      9   \n",
       "23056              55.666667             35.898004        41     10   \n",
       "\n",
       "       month_sin     month_cos  week_sin      week_cos  \n",
       "23047  -0.866025 -5.000000e-01 -0.663123 -7.485107e-01  \n",
       "23048  -0.866025 -5.000000e-01 -0.748511 -6.631227e-01  \n",
       "23049  -0.866025 -5.000000e-01 -0.822984 -5.680647e-01  \n",
       "23050  -0.866025 -5.000000e-01 -0.885456 -4.647232e-01  \n",
       "23051  -1.000000 -1.836970e-16 -0.935016 -3.546049e-01  \n",
       "23052  -1.000000 -1.836970e-16 -0.970942 -2.393157e-01  \n",
       "23053  -1.000000 -1.836970e-16 -0.992709 -1.205367e-01  \n",
       "23054  -1.000000 -1.836970e-16 -1.000000 -1.836970e-16  \n",
       "23055  -1.000000 -1.836970e-16 -0.992709  1.205367e-01  \n",
       "23056  -0.866025  5.000000e-01 -0.970942  2.393157e-01  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[(ds['product_id']==88208430684)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcd40c4-ec20-4d05-9fee-54fee8ecf6b4",
   "metadata": {},
   "source": [
    "# 함수로만  작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8824f119-6da1-4e84-921d-3325e001c50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prediction_length = 1  # 다음 주 예측\n",
    "max_encoder_length = 24    # 과거 12주 사용 (기존 24주에서 축소)\n",
    "min_encoder_length = 8     # 최소 8주의 데이터 필요\n",
    "\n",
    "\n",
    "def train_fillter(df):\n",
    "    # 데이터 준비 단계\n",
    "    # print(\"데이터 필터링 전:\")\n",
    "    # print(f\"전체 제품 수: {len(df_filled['product_id'].unique())}\")\n",
    "    \n",
    "    # 시퀀스 길이 계산\n",
    "    sequence_lengths = df_filled.groupby('product_id').size()\n",
    "    # print(\"\\n시퀀스 길이 통계:\")\n",
    "    # print(f\"최소 시퀀스 길이: {sequence_lengths.min()}\")\n",
    "    # print(f\"최대 시퀀스 길이: {sequence_lengths.max()}\")\n",
    "    # print(f\"평균 시퀀스 길이: {sequence_lengths.mean():.2f}\")\n",
    "\n",
    "    # 최소 필요 길이 설정\n",
    "    min_required_length = min_encoder_length + max_prediction_length\n",
    "    valid_products = sequence_lengths[sequence_lengths >= min_required_length].index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e286467d-aa2e-4afb-88a3-01ac145c4490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week_date</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>price</th>\n",
       "      <th>review_cnt</th>\n",
       "      <th>purchase_cnt</th>\n",
       "      <th>wish_cnt</th>\n",
       "      <th>sixMothRatio(puchase_cnt/review_cnt)</th>\n",
       "      <th>week_review_count</th>\n",
       "      <th>average_review_score</th>\n",
       "      <th>...</th>\n",
       "      <th>category2_encoded</th>\n",
       "      <th>category3_encoded</th>\n",
       "      <th>rolling_mean_purchase</th>\n",
       "      <th>rolling_std_purchase</th>\n",
       "      <th>week_num</th>\n",
       "      <th>month</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>week_sin</th>\n",
       "      <th>week_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-09-25</td>\n",
       "      <td>6356018199</td>\n",
       "      <td>DIY 목재재단 나무 원목 합판 집성목 MDF 방부목 자작나무 히노끼</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>61440.0</td>\n",
       "      <td>3745.0</td>\n",
       "      <td>1685.0</td>\n",
       "      <td>2.386871</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>68.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>14.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-10-02</td>\n",
       "      <td>6356018199</td>\n",
       "      <td>DIY 목재재단 나무 원목 합판 집성목 MDF 방부목 자작나무 히노끼</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>61440.0</td>\n",
       "      <td>3745.0</td>\n",
       "      <td>1685.0</td>\n",
       "      <td>2.386871</td>\n",
       "      <td>71.0</td>\n",
       "      <td>4.873239</td>\n",
       "      <td>...</td>\n",
       "      <td>68.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>91.50</td>\n",
       "      <td>109.601551</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-0.992709</td>\n",
       "      <td>1.205367e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-10-09</td>\n",
       "      <td>6356018199</td>\n",
       "      <td>DIY 목재재단 나무 원목 합판 집성목 MDF 방부목 자작나무 히노끼</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>61440.0</td>\n",
       "      <td>3745.0</td>\n",
       "      <td>1685.0</td>\n",
       "      <td>2.386871</td>\n",
       "      <td>97.0</td>\n",
       "      <td>4.742268</td>\n",
       "      <td>...</td>\n",
       "      <td>68.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>138.00</td>\n",
       "      <td>111.772090</td>\n",
       "      <td>41</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-0.970942</td>\n",
       "      <td>2.393157e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-10-16</td>\n",
       "      <td>6356018199</td>\n",
       "      <td>DIY 목재재단 나무 원목 합판 집성목 MDF 방부목 자작나무 히노끼</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>61440.0</td>\n",
       "      <td>3745.0</td>\n",
       "      <td>1685.0</td>\n",
       "      <td>2.386871</td>\n",
       "      <td>108.0</td>\n",
       "      <td>4.740741</td>\n",
       "      <td>...</td>\n",
       "      <td>68.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>167.75</td>\n",
       "      <td>108.944558</td>\n",
       "      <td>42</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-0.935016</td>\n",
       "      <td>3.546049e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-10-23</td>\n",
       "      <td>6356018199</td>\n",
       "      <td>DIY 목재재단 나무 원목 합판 집성목 MDF 방부목 자작나무 히노끼</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>61440.0</td>\n",
       "      <td>3745.0</td>\n",
       "      <td>1685.0</td>\n",
       "      <td>2.386871</td>\n",
       "      <td>89.0</td>\n",
       "      <td>4.853933</td>\n",
       "      <td>...</td>\n",
       "      <td>68.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>176.60</td>\n",
       "      <td>96.401763</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-0.885456</td>\n",
       "      <td>4.647232e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23211</th>\n",
       "      <td>2024-09-30</td>\n",
       "      <td>88459191933</td>\n",
       "      <td>모리모토 뉴스타 M1 내야 외야 투수 올라운드 야구 글러브 우투 좌투</td>\n",
       "      <td>79000.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.777778</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>23.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>-0.992709</td>\n",
       "      <td>1.205367e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23212</th>\n",
       "      <td>2024-10-07</td>\n",
       "      <td>88459191933</td>\n",
       "      <td>모리모토 뉴스타 M1 내야 외야 투수 올라운드 야구 글러브 우투 좌투</td>\n",
       "      <td>79000.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.777778</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>25.50</td>\n",
       "      <td>3.535534</td>\n",
       "      <td>41</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-0.970942</td>\n",
       "      <td>2.393157e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23213</th>\n",
       "      <td>2024-09-30</td>\n",
       "      <td>88473568229</td>\n",
       "      <td>노다지 영양 양곰탕 750g 7팩 곱창전골 내장탕 한우사골 즉석국 간편식</td>\n",
       "      <td>49900.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1088.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>8.845528</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>-0.992709</td>\n",
       "      <td>1.205367e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23214</th>\n",
       "      <td>2024-10-07</td>\n",
       "      <td>88473568229</td>\n",
       "      <td>노다지 영양 양곰탕 750g 7팩 곱창전골 내장탕 한우사골 즉석국 간편식</td>\n",
       "      <td>49900.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1088.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>8.845528</td>\n",
       "      <td>93.0</td>\n",
       "      <td>4.526882</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>415.00</td>\n",
       "      <td>575.584920</td>\n",
       "      <td>41</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-0.970942</td>\n",
       "      <td>2.393157e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23215</th>\n",
       "      <td>2024-10-14</td>\n",
       "      <td>88473568229</td>\n",
       "      <td>노다지 영양 양곰탕 750g 7팩 곱창전골 내장탕 한우사골 즉석국 간편식</td>\n",
       "      <td>49900.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1088.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>8.845528</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4.551724</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>362.00</td>\n",
       "      <td>417.224160</td>\n",
       "      <td>42</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-0.935016</td>\n",
       "      <td>3.546049e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23216 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        week_date   product_id                              product_name  \\\n",
       "0      2023-09-25   6356018199    DIY 목재재단 나무 원목 합판 집성목 MDF 방부목 자작나무 히노끼   \n",
       "1      2023-10-02   6356018199    DIY 목재재단 나무 원목 합판 집성목 MDF 방부목 자작나무 히노끼   \n",
       "2      2023-10-09   6356018199    DIY 목재재단 나무 원목 합판 집성목 MDF 방부목 자작나무 히노끼   \n",
       "3      2023-10-16   6356018199    DIY 목재재단 나무 원목 합판 집성목 MDF 방부목 자작나무 히노끼   \n",
       "4      2023-10-23   6356018199    DIY 목재재단 나무 원목 합판 집성목 MDF 방부목 자작나무 히노끼   \n",
       "...           ...          ...                                       ...   \n",
       "23211  2024-09-30  88459191933    모리모토 뉴스타 M1 내야 외야 투수 올라운드 야구 글러브 우투 좌투   \n",
       "23212  2024-10-07  88459191933    모리모토 뉴스타 M1 내야 외야 투수 올라운드 야구 글러브 우투 좌투   \n",
       "23213  2024-09-30  88473568229  노다지 영양 양곰탕 750g 7팩 곱창전골 내장탕 한우사골 즉석국 간편식   \n",
       "23214  2024-10-07  88473568229  노다지 영양 양곰탕 750g 7팩 곱창전골 내장탕 한우사골 즉석국 간편식   \n",
       "23215  2024-10-14  88473568229  노다지 영양 양곰탕 750g 7팩 곱창전골 내장탕 한우사골 즉석국 간편식   \n",
       "\n",
       "         price  review_cnt  purchase_cnt  wish_cnt  \\\n",
       "0       1000.0     61440.0        3745.0    1685.0   \n",
       "1       1000.0     61440.0        3745.0    1685.0   \n",
       "2       1000.0     61440.0        3745.0    1685.0   \n",
       "3       1000.0     61440.0        3745.0    1685.0   \n",
       "4       1000.0     61440.0        3745.0    1685.0   \n",
       "...        ...         ...           ...       ...   \n",
       "23211  79000.0         8.0          52.0      25.0   \n",
       "23212  79000.0         8.0          52.0      25.0   \n",
       "23213  49900.0        70.0        1088.0     101.0   \n",
       "23214  49900.0        70.0        1088.0     101.0   \n",
       "23215  49900.0        70.0        1088.0     101.0   \n",
       "\n",
       "       sixMothRatio(puchase_cnt/review_cnt)  week_review_count  \\\n",
       "0                                  2.386871                6.0   \n",
       "1                                  2.386871               71.0   \n",
       "2                                  2.386871               97.0   \n",
       "3                                  2.386871              108.0   \n",
       "4                                  2.386871               89.0   \n",
       "...                                     ...                ...   \n",
       "23211                              5.777778                4.0   \n",
       "23212                              5.777778                5.0   \n",
       "23213                              8.845528                1.0   \n",
       "23214                              8.845528               93.0   \n",
       "23215                              8.845528               29.0   \n",
       "\n",
       "       average_review_score  ...  category2_encoded  category3_encoded  \\\n",
       "0                  4.666667  ...               68.0               99.0   \n",
       "1                  4.873239  ...               68.0               99.0   \n",
       "2                  4.742268  ...               68.0               99.0   \n",
       "3                  4.740741  ...               68.0               99.0   \n",
       "4                  4.853933  ...               68.0               99.0   \n",
       "...                     ...  ...                ...                ...   \n",
       "23211              4.750000  ...               13.0              184.0   \n",
       "23212              4.800000  ...               13.0              184.0   \n",
       "23213              3.000000  ...                5.0              202.0   \n",
       "23214              4.526882  ...                5.0              202.0   \n",
       "23215              4.551724  ...                5.0              202.0   \n",
       "\n",
       "       rolling_mean_purchase  rolling_std_purchase  week_num  month  \\\n",
       "0                      14.00              0.000000        39      9   \n",
       "1                      91.50            109.601551        40     10   \n",
       "2                     138.00            111.772090        41     10   \n",
       "3                     167.75            108.944558        42     10   \n",
       "4                     176.60             96.401763        43     10   \n",
       "...                      ...                   ...       ...    ...   \n",
       "23211                  23.00              0.000000        40      9   \n",
       "23212                  25.50              3.535534        41     10   \n",
       "23213                   8.00              0.000000        40      9   \n",
       "23214                 415.00            575.584920        41     10   \n",
       "23215                 362.00            417.224160        42     10   \n",
       "\n",
       "       month_sin     month_cos  week_sin      week_cos  \n",
       "0      -1.000000 -1.836970e-16 -1.000000 -1.836970e-16  \n",
       "1      -0.866025  5.000000e-01 -0.992709  1.205367e-01  \n",
       "2      -0.866025  5.000000e-01 -0.970942  2.393157e-01  \n",
       "3      -0.866025  5.000000e-01 -0.935016  3.546049e-01  \n",
       "4      -0.866025  5.000000e-01 -0.885456  4.647232e-01  \n",
       "...          ...           ...       ...           ...  \n",
       "23211  -1.000000 -1.836970e-16 -0.992709  1.205367e-01  \n",
       "23212  -0.866025  5.000000e-01 -0.970942  2.393157e-01  \n",
       "23213  -1.000000 -1.836970e-16 -0.992709  1.205367e-01  \n",
       "23214  -0.866025  5.000000e-01 -0.970942  2.393157e-01  \n",
       "23215  -0.866025  5.000000e-01 -0.935016  3.546049e-01  \n",
       "\n",
       "[23216 rows x 22 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "week_date                                object\n",
    "product_id                                int64\n",
    "category1Id                             float64\n",
    "category2Id                             float64\n",
    "category3Id                             float64\n",
    "price                                   float64\n",
    "review_cnt                              float64\n",
    "purchase_cnt                            float64\n",
    "wish_cnt                                float64\n",
    "sixMothRatio(puchase_cnt/review_cnt)    float64\n",
    "week_review_count                       float64\n",
    "average_review_score                    float64\n",
    "week_purchase_cnt                       float64\n",
    "dtype: object\n",
    "'''\n",
    "\n",
    "df_filled = ds.copy()\n",
    "df_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "44c3f716-e856-4701-bce6-5dc875bdeff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week_date                                object\n",
      "product_id                                int64\n",
      "product_name                             object\n",
      "price                                   float64\n",
      "review_cnt                              float64\n",
      "purchase_cnt                            float64\n",
      "wish_cnt                                float64\n",
      "sixMothRatio(puchase_cnt/review_cnt)    float64\n",
      "week_review_count                       float64\n",
      "average_review_score                    float64\n",
      "week_purchase_cnt                       float64\n",
      "category1_encoded                       float64\n",
      "category2_encoded                       float64\n",
      "category3_encoded                       float64\n",
      "rolling_mean_purchase                   float64\n",
      "rolling_std_purchase                    float64\n",
      "week_num                                  int64\n",
      "month                                     int64\n",
      "month_sin                               float64\n",
      "month_cos                               float64\n",
      "week_sin                                float64\n",
      "week_cos                                float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_filled.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "25c67a8d-ece0-4e2d-bf8b-29425012ae3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (24.2)\n",
      "Collecting pip\n",
      "  Using cached pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Using cached pip-24.3.1-py3-none-any.whl (1.8 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.2\n",
      "    Uninstalling pip-24.2:\n",
      "      Successfully uninstalled pip-24.2\n",
      "Successfully installed pip-24.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5566fcd7-2191-4a64-91d7-8a0fbd666eba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-lightning in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.4.0)\n",
      "Requirement already satisfied: pytorch-forecasting in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.2.0)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-lightning) (4.66.6)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-lightning) (6.0.2)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.10.0)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-lightning) (1.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-lightning) (23.1)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /Users/jenzennii/Library/Python/3.10/lib/python/site-packages (from pytorch-lightning) (4.12.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-lightning) (0.11.8)\n",
      "Requirement already satisfied: numpy<=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-forecasting) (1.24.3)\n",
      "Requirement already satisfied: lightning<3.0.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-forecasting) (2.4.0)\n",
      "Requirement already satisfied: scipy<2.0,>=1.8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-forecasting) (1.14.1)\n",
      "Requirement already satisfied: pandas<3.0.0,>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-forecasting) (2.1.1)\n",
      "Requirement already satisfied: scikit-learn<2.0,>=1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-forecasting) (1.5.2)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.10.10)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (65.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn<2.0,>=1.2->pytorch-forecasting) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn<2.0,>=1.2->pytorch-forecasting) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.17.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=1.3.0->pytorch-forecasting) (1.16.0)\n",
      "Requirement already satisfied: idna>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pytorch-lightning pytorch-forecasting torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfea31fe-6af9-40ca-90c7-340ba0bc57d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.5.1\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3-Clause\n",
      "Location: /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, sympy, typing-extensions\n",
      "Required-by: lightning, pytorch-forecasting, pytorch-lightning, sentence-transformers, torchaudio, torchmetrics, torchvision\n",
      "---\n",
      "Name: pytorch-lightning\n",
      "Version: 2.4.0\n",
      "Summary: PyTorch Lightning is the lightweight PyTorch wrapper for ML researchers. Scale your models. Write less boilerplate.\n",
      "Home-page: https://github.com/Lightning-AI/lightning\n",
      "Author: Lightning AI et al.\n",
      "Author-email: pytorch@lightning.ai\n",
      "License: Apache-2.0\n",
      "Location: /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages\n",
      "Requires: fsspec, lightning-utilities, packaging, PyYAML, torch, torchmetrics, tqdm, typing-extensions\n",
      "Required-by: lightning\n",
      "---\n",
      "Name: pytorch-forecasting\n",
      "Version: 1.2.0\n",
      "Summary: Forecasting timeseries with PyTorch - dataloaders, normalizers, metrics and models\n",
      "Home-page: \n",
      "Author: Jan Beitner\n",
      "Author-email: \n",
      "License: \n",
      "Location: /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages\n",
      "Requires: lightning, numpy, pandas, scikit-learn, scipy, torch\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show torch pytorch-lightning pytorch-forecasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f590713-66fb-488b-8218-3c21320cbc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-forecasting in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy<=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-forecasting) (1.24.3)\n",
      "Requirement already satisfied: torch!=2.0.1,<3.0.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-forecasting) (2.5.1)\n",
      "Requirement already satisfied: lightning<3.0.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-forecasting) (2.4.0)\n",
      "Requirement already satisfied: scipy<2.0,>=1.8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-forecasting) (1.14.1)\n",
      "Requirement already satisfied: pandas<3.0.0,>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-forecasting) (2.1.1)\n",
      "Requirement already satisfied: scikit-learn<2.0,>=1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-forecasting) (1.5.2)\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (6.0.2)\n",
      "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (2024.10.0)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (0.11.8)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (23.1)\n",
      "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.5.1)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (4.66.6)\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /Users/jenzennii/Library/Python/3.10/lib/python/site-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (4.12.2)\n",
      "Requirement already satisfied: pytorch-lightning in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn<2.0,>=1.2->pytorch-forecasting) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn<2.0,>=1.2->pytorch-forecasting) (3.5.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (3.16.1)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (3.1.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy==1.13.1->torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (1.3.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (3.10.10)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.10.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (65.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=1.3.0->pytorch-forecasting) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (2.1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.17.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (4.0.3)\n",
      "Requirement already satisfied: idna>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (3.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pytorch-forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbae4b5b-2d5b-4e35-bc1f-cce813e07e54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.5.1\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3-Clause\n",
      "Location: /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, sympy, typing-extensions\n",
      "Required-by: lightning, pytorch-forecasting, pytorch-lightning, sentence-transformers, torchaudio, torchmetrics, torchvision\n",
      "---\n",
      "Name: pytorch-lightning\n",
      "Version: 2.4.0\n",
      "Summary: PyTorch Lightning is the lightweight PyTorch wrapper for ML researchers. Scale your models. Write less boilerplate.\n",
      "Home-page: https://github.com/Lightning-AI/lightning\n",
      "Author: Lightning AI et al.\n",
      "Author-email: pytorch@lightning.ai\n",
      "License: Apache-2.0\n",
      "Location: /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages\n",
      "Requires: fsspec, lightning-utilities, packaging, PyYAML, torch, torchmetrics, tqdm, typing-extensions\n",
      "Required-by: lightning\n",
      "---\n",
      "Name: pytorch-forecasting\n",
      "Version: 1.2.0\n",
      "Summary: Forecasting timeseries with PyTorch - dataloaders, normalizers, metrics and models\n",
      "Home-page: \n",
      "Author: Jan Beitner\n",
      "Author-email: \n",
      "License: \n",
      "Location: /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages\n",
      "Requires: lightning, numpy, pandas, scikit-learn, scipy, torch\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show torch pytorch-lightning pytorch-forecasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ee3c471-863a-4914-8ea8-1f00fe0407d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-lightning==1.6.0\n",
      "  Downloading pytorch_lightning-1.6.0-py3-none-any.whl.metadata (33 kB)\n",
      "\u001b[33mWARNING: Ignoring version 1.6.0 of pytorch-lightning since it has invalid metadata:\n",
      "Requested pytorch-lightning==1.6.0 from https://files.pythonhosted.org/packages/09/18/cee67f4849dea9a29b7af7cdf582246bcba9eaa73d9443e138a4172ec786/pytorch_lightning-1.6.0-py3-none-any.whl has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
      "    torch (>=1.8.*)\n",
      "           ~~~~~~^\n",
      "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement pytorch-lightning==1.6.0 (from versions: 0.0.2, 0.2, 0.2.2, 0.2.3, 0.2.4, 0.2.4.1, 0.2.5, 0.2.5.1, 0.2.5.2, 0.2.6, 0.3, 0.3.1, 0.3.2, 0.3.3, 0.3.4, 0.3.4.1, 0.3.5, 0.3.6, 0.3.6.1, 0.3.6.3, 0.3.6.4, 0.3.6.5, 0.3.6.6, 0.3.6.7, 0.3.6.8, 0.3.6.9, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.4.4, 0.4.5, 0.4.6, 0.4.7, 0.4.8, 0.4.9, 0.5.0, 0.5.1, 0.5.1.2, 0.5.1.3, 0.5.2, 0.5.2.1, 0.5.3, 0.5.3.1, 0.5.3.2, 0.5.3.3, 0.6.0, 0.7.1, 0.7.3, 0.7.5, 0.7.6, 0.8.1, 0.8.3, 0.8.4, 0.8.5, 0.9.0, 0.10.0, 1.0.0, 1.0.1, 1.0.2, 1.0.3, 1.0.4, 1.0.5, 1.0.6, 1.0.7, 1.0.8, 1.1.0, 1.1.1, 1.1.2, 1.1.3, 1.1.4, 1.1.5, 1.1.6, 1.1.7, 1.1.8, 1.2.0rc0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.2.4, 1.2.5, 1.2.6, 1.2.7, 1.2.8, 1.2.9, 1.2.10, 1.3.0rc1, 1.3.0rc2, 1.3.0rc3, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.3.4, 1.3.5, 1.3.6, 1.3.7, 1.3.7.post0, 1.3.8, 1.4.0rc0, 1.4.0rc1, 1.4.0rc2, 1.4.0, 1.4.1, 1.4.2, 1.4.3, 1.4.4, 1.4.5, 1.4.6, 1.4.7, 1.4.8, 1.4.9, 1.5.0rc0, 1.5.0rc1, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.5.5, 1.5.6, 1.5.7, 1.5.8, 1.5.9, 1.5.10, 1.6.0rc0, 1.6.0rc1, 1.6.0, 1.6.1, 1.6.2, 1.6.3, 1.6.4, 1.6.5, 1.7.0rc0, 1.7.0rc1, 1.7.0, 1.7.1, 1.7.2, 1.7.3, 1.7.4, 1.7.5, 1.7.6, 1.7.7, 1.8.0rc0, 1.8.0rc1, 1.8.0rc2, 1.8.0, 1.8.0.post1, 1.8.1, 1.8.2, 1.8.3, 1.8.3.post0, 1.8.3.post1, 1.8.3.post2, 1.8.4, 1.8.4.post0, 1.8.5, 1.8.5.post0, 1.8.6, 1.9.0rc0, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.9.4, 1.9.5, 2.0.0rc0, 2.0.0, 2.0.1, 2.0.1.post0, 2.0.2, 2.0.3, 2.0.4, 2.0.5, 2.0.6, 2.0.7, 2.0.8, 2.0.9, 2.0.9.post0, 2.1.0rc0, 2.1.0rc1, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.1.4, 2.2.0rc0, 2.2.0, 2.2.0.post0, 2.2.1, 2.2.2, 2.2.3, 2.2.4, 2.2.5, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.4.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pytorch-lightning==1.6.0\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytorch-lightning==1.6.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "656631c1-3185-4323-b02a-0add5d9104de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pytorch_forecasting/data/timeseries.py:1301: UserWarning: Min encoder length and/or min_prediction_idx and/or min prediction length and/or lags are too large for 3 series/groups which therefore are not present in the dataset index. This means no predictions can be made for those series. First 10 removed groups: [{'__group_id__product_id': '88222671639'}, {'__group_id__product_id': '88284816214'}, {'__group_id__product_id': '88291977922'}]\n",
      "  warnings.warn(\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 필터링 전:\n",
      "전체 제품 수: 453\n",
      "\n",
      "시퀀스 길이 통계:\n",
      "최소 시퀀스 길이: 2\n",
      "최대 시퀀스 길이: 310\n",
      "평균 시퀀스 길이: 51.25\n",
      "\n",
      "조정된 최소 필요 데이터 길이: 9\n",
      "충분한 데이터를 가진 제품 수: 431\n",
      "포함된 제품 비율: 95.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py:171: UserWarning: In pytorch-forecasting models, on versions 1.1.X, the default optimizer defaults to 'adam', if pytorch_optimizer is not installed, otherwise it defaults to 'ranger' from pytorch_optimizer. From version 1.2.0, the default optimizer will be 'adam' regardless of whether pytorch_optimizer is installed, in order to minimize the number of dependencies in default parameter settings. Users who wish to ensure their code continues using 'ranger' as optimizer should ensure that pytorch_optimizer is installed, and set the optimizer parameter explicitly to 'ranger'.\n",
      "  super().__init__(loss=loss, logging_metrics=logging_metrics, **kwargs)\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | RMSE                            | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 13.8 K | train\n",
      "3  | prescalers                         | ModuleDict                      | 736    | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 6.0 K  | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 44.9 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 42.3 K | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 8.4 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 8.4 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K  | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 64     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 3.2 K  | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K  | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K  | train\n",
      "20 | output_layer                       | Linear                          | 33     | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "159 K     Trainable params\n",
      "0         Non-trainable params\n",
      "159 K     Total params\n",
      "0.639     Total estimated model params size (MB)\n",
      "772       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "모델 학습 시작...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c84550459df4ba0bf942967f35bed49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py:250\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m     batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:190\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:268\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:167\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 167\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/core/module.py:1306\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;124;03mthe optimizer.\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1304\u001b[0m \n\u001b[1;32m   1305\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1306\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:153\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:238\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py:122\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/optim/adam.py:202\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 202\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py:108\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03mhook is called.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:144\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:138\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:239\u001b[0m, in \u001b[0;36m_AutomaticOptimization._make_backward_fn.<locals>.backward_fn\u001b[0;34m(loss)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward_fn\u001b[39m(loss: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m     \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:319\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 319\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:212\u001b[0m, in \u001b[0;36mStrategy.backward\u001b[0;34m(self, closure_loss, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m closure_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mpre_backward(closure_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module)\n\u001b[0;32m--> 212\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m closure_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mpost_backward(closure_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py:72\u001b[0m, in \u001b[0;36mPrecision.backward\u001b[0;34m(self, tensor, model, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Performs the actual backpropagation.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/core/module.py:1101\u001b[0m, in \u001b[0;36mLightningModule.backward\u001b[0;34m(self, loss, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1101\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 153\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m모델 학습 시작...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 153\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# val_dataloaders=val_dataloader\u001b[39;49;00m\n\u001b[1;32m    157\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# 검증 세트에 대한 성능 평가\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m검증 세트 성능 평가:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, QuantileLoss\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import RMSE\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# 기존 파라미터 조정\n",
    "max_prediction_length = 1  # 다음 주 예측\n",
    "max_encoder_length = 24    # 과거 12주 사용 (기존 24주에서 축소)\n",
    "min_encoder_length = 8     # 최소 8주의 데이터 필요\n",
    "\n",
    "features = [ \n",
    "             'price',\n",
    "             'review_cnt',\n",
    "             'wish_cnt',\n",
    "             'sixMothRatio(puchase_cnt/review_cnt)',\n",
    "             'week_review_count',\n",
    "             'average_review_score',\n",
    "             'category1_encoded',\n",
    "             'category2_encoded',\n",
    "             'category3_encoded',\n",
    "             'rolling_mean_purchase',\n",
    "             'rolling_std_purchase',\n",
    "             'week_num',\n",
    "             'month',\n",
    "             'month_sin',\n",
    "             'month_cos',\n",
    "             'week_sin',\n",
    "             'week_cos']\n",
    "\n",
    "\n",
    "\n",
    "# 데이터 준비 단계\n",
    "print(\"데이터 필터링 전:\")\n",
    "print(f\"전체 제품 수: {len(df_filled['product_id'].unique())}\")\n",
    "\n",
    "# 시퀀스 길이 계산\n",
    "sequence_lengths = df_filled.groupby('product_id').size()\n",
    "print(\"\\n시퀀스 길이 통계:\")\n",
    "print(f\"최소 시퀀스 길이: {sequence_lengths.min()}\")\n",
    "print(f\"최대 시퀀스 길이: {sequence_lengths.max()}\")\n",
    "print(f\"평균 시퀀스 길이: {sequence_lengths.mean():.2f}\")\n",
    "\n",
    "# 최소 필요 길이 설정\n",
    "min_required_length = min_encoder_length + max_prediction_length\n",
    "valid_products = sequence_lengths[sequence_lengths >= min_required_length].index\n",
    "\n",
    "print(f\"\\n조정된 최소 필요 데이터 길이: {min_required_length}\")\n",
    "print(f\"충분한 데이터를 가진 제품 수: {len(valid_products)}\")\n",
    "print(f\"포함된 제품 비율: {(len(valid_products) / len(sequence_lengths) * 100):.2f}%\")\n",
    "\n",
    "\n",
    "# 유효한 제품만 필터링\n",
    "df_filtered = df_filled[df_filled['product_id'].isin(valid_products)].copy()\n",
    "\n",
    "\n",
    "# time_idx 재생성\n",
    "# week_date를 datetime 형식으로 변환\n",
    "df_filtered ['week_date'] = pd.to_datetime(df_filtered ['week_date'])\n",
    "# product_id를 문자열로 변환\n",
    "df_filtered ['product_id'] = df_filtered ['product_id'].astype(str)\n",
    "# 각 product_id마다 0부터 시작하는 time_idx 생성 -> 상품 별로 시계열 예측을 진행하도록 함.\n",
    "df_filtered = df_filtered.sort_values([\"product_id\", \"week_date\"]).reset_index(drop=True)\n",
    "df_filtered[\"time_idx\"] = df_filtered.groupby(\"product_id\").cumcount()\n",
    "\n",
    "# training cutoff 설정 (각 시퀀스의 80%를 훈련에 사용)\n",
    "df_filtered[\"training_cutoff\"] = df_filtered.groupby(\"product_id\")[\"time_idx\"].transform(\n",
    "    lambda x: int(len(x) * 0.8)\n",
    ")\n",
    "\n",
    "# print(df_filtered.dtypes)\n",
    "\n",
    "# Training dataset 생성\n",
    "training = TimeSeriesDataSet(\n",
    "    df_filtered[lambda x: x.time_idx <= x.training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"week_purchase_cnt\",\n",
    "    group_ids=[\"product_id\"],\n",
    "    min_encoder_length=min_encoder_length,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"product_id\"],\n",
    "    time_varying_known_reals=[\"time_idx\"] + features,\n",
    "    time_varying_unknown_reals=[\"week_purchase_cnt\"],\n",
    "    target_normalizer=GroupNormalizer(groups=[\"product_id\"]),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "# Validation dataset 생성\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training,\n",
    "    df_filtered,\n",
    "    min_prediction_idx=training.index.time.max() + 1,\n",
    "    stop_randomization=True\n",
    ")\n",
    "\n",
    "# 데이터 로더 생성\n",
    "batch_size = 128  # 배치 크기 증가\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 2, num_workers=0)\n",
    "\n",
    "\n",
    "# Early stopping 설정 조정\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=1e-4,\n",
    "    patience=10,\n",
    "    verbose=True,\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "# 모델 파라미터 조정\n",
    "# 모델 초기화\n",
    "pl.seed_everything(42)\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=32,\n",
    "    attention_head_size=2,\n",
    "    dropout=0.2,  # 드롭아웃 증가\n",
    "    hidden_continuous_size=16,\n",
    "    loss=RMSE(),\n",
    ")\n",
    "\n",
    "# Trainer 초기화\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    devices=1,\n",
    "    accelerator='gpu',\n",
    "    gradient_clip_val=0.1,\n",
    "    callbacks=[early_stop_callback, lr_monitor],\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "\n",
    "# 모델 학습\n",
    "print(\"\\n모델 학습 시작...\")\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    # val_dataloaders=val_dataloader\n",
    ")\n",
    "\n",
    "# 검증 세트에 대한 성능 평가\n",
    "print(\"\\n검증 세트 성능 평가:\")\n",
    "validation_predictions = tft.predict(val_dataloader)\n",
    "validation_actual = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "\n",
    "# GPU 텐서를 CPU로 이동하여 계산\n",
    "val_predictions_np = validation_predictions.cpu().numpy()\n",
    "val_actual_np = validation_actual.cpu().numpy()\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(val_actual_np, val_predictions_np))\n",
    "r2 = r2_score(val_actual_np, val_predictions_np)\n",
    "\n",
    "print(f\"Validation RMSE: {rmse:.2f}\")\n",
    "print(f\"Validation R²: {r2:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412a4d95-f7fc-4915-b4ed-c314ba68857b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# class화로 self로 받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3cf296d-3cad-416a-b268-580b26b5b701",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pytorch_forecasting/data/timeseries.py:1282: UserWarning: Min encoder length and/or min_prediction_idx and/or min prediction length and/or lags are too large for 25 series/groups which therefore are not present in the dataset index. This means no predictions can be made for those series. First 10 removed groups: [{'__group_id__product_id': '82562973333'}, {'__group_id__product_id': '87149885404'}, {'__group_id__product_id': '87226364197'}, {'__group_id__product_id': '87817018176'}, {'__group_id__product_id': '87844396267'}, {'__group_id__product_id': '87896780670'}, {'__group_id__product_id': '87921020002'}, {'__group_id__product_id': '87937248809'}, {'__group_id__product_id': '87952389253'}, {'__group_id__product_id': '87958278370'}]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Unknown category '88208430684' encountered. Set `add_nan=True` to allow unknown categories\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pytorch_forecasting/data/encoders.py:327\u001b[0m, in \u001b[0;36mNaNLabelEncoder.transform\u001b[0;34m(self, y, return_norm, target_scale, ignore_na)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m     encoded \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m y]\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pytorch_forecasting/data/encoders.py:327\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m     encoded \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m y]\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyError\u001b[0m: '88208430684'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 223\u001b[0m\n\u001b[1;32m    220\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# 모델 인스턴스 생성 및 학습\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m tft_model \u001b[38;5;241m=\u001b[39m \u001b[43mTFTModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmax_encoder_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_prediction_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m tft_model\u001b[38;5;241m.\u001b[39mfit()\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# 평가 및 예측\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[29], line 50\u001b[0m, in \u001b[0;36mTFTModel.__init__\u001b[0;34m(self, data_path, features, target_column, max_encoder_length, max_prediction_length, batch_size, checkpoint_dir, model_save_dir)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m TimeSeriesDataSet(\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_df[\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mtime_idx \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_cutoff],\n\u001b[1;32m     37\u001b[0m     time_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     target_normalizer\u001b[38;5;241m=\u001b[39mGroupNormalizer(groups\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m     47\u001b[0m )\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# validation 데이터셋 생성\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation \u001b[38;5;241m=\u001b[39m \u001b[43mTimeSeriesDataSet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop_randomization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     55\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# 데이터 로더 준비\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining\u001b[38;5;241m.\u001b[39mto_dataloader(train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pytorch_forecasting/data/timeseries.py:1155\u001b[0m, in \u001b[0;36mTimeSeriesDataSet.from_dataset\u001b[0;34m(cls, dataset, data, stop_randomization, predict, **update_kwargs)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_dataset\u001b[39m(\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;28mcls\u001b[39m, dataset, data: pd\u001b[38;5;241m.\u001b[39mDataFrame, stop_randomization: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, predict: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mupdate_kwargs\n\u001b[1;32m   1137\u001b[0m ):\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;124;03m    Generate dataset with different underlying data but same variable encoders and scalers, etc.\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;124;03m        TimeSeriesDataSet: new dataset\u001b[39;00m\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_parameters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_randomization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop_randomization\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mupdate_kwargs\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pytorch_forecasting/data/timeseries.py:1201\u001b[0m, in \u001b[0;36mTimeSeriesDataSet.from_parameters\u001b[0;34m(cls, parameters, data, stop_randomization, predict, **update_kwargs)\u001b[0m\n\u001b[1;32m   1198\u001b[0m     parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandomize_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1199\u001b[0m parameters\u001b[38;5;241m.\u001b[39mupdate(update_kwargs)\n\u001b[0;32m-> 1201\u001b[0m new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pytorch_forecasting/data/timeseries.py:477\u001b[0m, in \u001b[0;36mTimeSeriesDataSet.__init__\u001b[0;34m(self, data, time_idx, target, group_ids, weight, max_encoder_length, min_encoder_length, min_prediction_idx, min_prediction_length, max_prediction_length, static_categoricals, static_reals, time_varying_known_categoricals, time_varying_known_reals, time_varying_unknown_categoricals, time_varying_unknown_reals, variable_groups, constant_fill_strategy, allow_missing_timesteps, lags, add_relative_time_idx, add_target_scales, add_encoder_length, target_normalizer, categorical_encoders, scalers, randomize_length, predict_mode)\u001b[0m\n\u001b[1;32m    474\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_ids \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_idx])\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# preprocess data\u001b[39;00m\n\u001b[0;32m--> 477\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_names:\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m target \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscalers, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget normalizer is separate and not in scalers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pytorch_forecasting/data/timeseries.py:734\u001b[0m, in \u001b[0;36mTimeSeriesDataSet._preprocess_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mfromkeys(group_ids_to_encode \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflat_categoricals):\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;66;03m# targets and its lagged versions are handled separetely\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_names \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlagged_targets:\n\u001b[0;32m--> 734\u001b[0m         data[name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlagged_variables\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;66;03m# save special variables\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__time_idx__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mcolumns, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__time_idx__ is a protected column and must not be present in data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pytorch_forecasting/data/timeseries.py:936\u001b[0m, in \u001b[0;36mTimeSeriesDataSet.transform_values\u001b[0;34m(self, name, values, data, inverse, group_id, **kwargs)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;66;03m# remaining categories\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflat_categoricals \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_ids \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group_ids:\n\u001b[0;32m--> 936\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# reals\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreals:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    322\u001b[0m         )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pytorch_forecasting/data/encoders.py:329\u001b[0m, in \u001b[0;36mNaNLabelEncoder.transform\u001b[0;34m(self, y, return_norm, target_scale, ignore_na)\u001b[0m\n\u001b[1;32m    327\u001b[0m             encoded \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m y]\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 329\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m    330\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown category \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m encountered. Set `add_nan=True` to allow unknown categories\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    331\u001b[0m             )\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    334\u001b[0m     encoded \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(encoded, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Unknown category '88208430684' encountered. Set `add_nan=True` to allow unknown categories\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import RMSE\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class TFTModel:\n",
    "    def __init__(self, data_path, features, target_column, max_encoder_length, max_prediction_length, batch_size, checkpoint_dir=\"./checkpoints\", model_save_dir=\"./weights\"):\n",
    "        # 데이터 로드 및 전처리\n",
    "        self.df = pd.read_csv(data_path)\n",
    "        self.features = features\n",
    "        self.target_column = target_column\n",
    "        self.max_encoder_length = max_encoder_length\n",
    "        self.max_prediction_length = max_prediction_length\n",
    "        self.batch_size = batch_size\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.model_save_dir = model_save_dir\n",
    "\n",
    "        self.df['product_id'] = self.df['product_id'].astype(str)\n",
    "        self.df['week_date'] = pd.to_datetime(self.df['week_date'])\n",
    "        self.df = self.df.sort_values(\"week_date\").reset_index(drop=True)\n",
    "        self.df[\"time_idx\"] = (self.df[\"week_date\"] - self.df[\"week_date\"].min()).dt.days // 7  # 주 단위로 인덱스 생성\n",
    "\n",
    "        # 8:2로 train과 validation 분리\n",
    "        self.train_df, self.val_df = train_test_split(self.df, test_size=0.2, shuffle=False)\n",
    "        \n",
    "        # 훈련 데이터 준비\n",
    "        self.training_cutoff = self.train_df['time_idx'].max() - max_prediction_length\n",
    "        self.training = TimeSeriesDataSet(\n",
    "            self.train_df[lambda x: x.time_idx <= self.training_cutoff],\n",
    "            time_idx=\"time_idx\",\n",
    "            target=self.target_column,\n",
    "            group_ids=[\"product_id\"],\n",
    "            min_encoder_length=max_encoder_length // 2,\n",
    "            max_encoder_length=max_encoder_length,\n",
    "            max_prediction_length=max_prediction_length,\n",
    "            static_categoricals=[\"product_id\"],\n",
    "            time_varying_known_reals=[\"time_idx\"] + features,\n",
    "            time_varying_unknown_reals=[target_column],\n",
    "            target_normalizer=GroupNormalizer(groups=[\"product_id\"]),\n",
    "        )\n",
    "\n",
    "        # validation 데이터셋 생성\n",
    "        self.validation = TimeSeriesDataSet.from_dataset(\n",
    "            self.training,\n",
    "            self.train_df,\n",
    "            predict=True,\n",
    "            stop_randomization=True\n",
    "        )\n",
    "\n",
    "        # 데이터 로더 준비\n",
    "        self.train_dataloader = self.training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "        self.val_dataloader = self.validation.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "        # 모델 정의\n",
    "        self.tft = TemporalFusionTransformer.from_dataset(\n",
    "            self.training,\n",
    "            learning_rate=0.03,\n",
    "            hidden_size=16,\n",
    "            attention_head_size=1,\n",
    "            dropout=0.1,\n",
    "            hidden_continuous_size=8,\n",
    "            output_size=1,  # max_prediction_length와 동일하게 설정\n",
    "            loss=RMSE(),\n",
    "        )\n",
    "\n",
    "        # Trainer 설정\n",
    "        self.trainer = pl.Trainer(\n",
    "            max_epochs=10,\n",
    "            accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "            devices=1,\n",
    "            gradient_clip_val=0.1,\n",
    "            logger=pl.loggers.TensorBoardLogger('tb_logs'),\n",
    "            callbacks=[pl.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)]\n",
    "        )\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        학습을 진행하는 함수\n",
    "        \"\"\"\n",
    "        # 모델 학습\n",
    "        pl.seed_everything(42)\n",
    "        self.trainer.fit(\n",
    "            model=self.tft,\n",
    "            train_dataloaders=self.train_dataloader,\n",
    "            val_dataloaders=self.val_dataloader\n",
    "        )\n",
    "\n",
    "        # 체크포인트 저장\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "        checkpoint_path = os.path.join(self.checkpoint_dir, \"tft_model_checkpoint.ckpt\")\n",
    "        self.trainer.save_checkpoint(checkpoint_path)\n",
    "\n",
    "        # 모델 가중치 저장\n",
    "        os.makedirs(self.model_save_dir, exist_ok=True)\n",
    "        model_save_path = os.path.join(self.model_save_dir, \"tft_model_weights.pth\")\n",
    "        torch.save(self.tft.state_dict(), model_save_path)\n",
    "\n",
    "        print(\"모델 학습 완료 및 저장.\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        validation 데이터를 사용하여 모델 성능 평가\n",
    "        \"\"\"\n",
    "        predictions = self.trainer.predict(self.tft, self.val_dataloader)\n",
    "        predictions = torch.cat([p.prediction for p in predictions]).cpu().numpy()\n",
    "\n",
    "        # 실제 값과 예측값 비교\n",
    "        actuals = torch.cat([self.tft.to_network_output(batch)[0] for batch in iter(self.val_dataloader)]).cpu().numpy()\n",
    "        \n",
    "        mse = mean_squared_error(actuals, predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(actuals, predictions)\n",
    "        \n",
    "        print(f\"Validation RMSE: {rmse:.4f}, R2: {r2:.4f}\")\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def plot_predictions(self, product_ids):\n",
    "        \"\"\"\n",
    "        각 상품별 예측 결과를 실제 값과 함께 시각화\n",
    "        \"\"\"\n",
    "        predictions = self.trainer.predict(self.tft, self.val_dataloader)\n",
    "        predictions = torch.cat([p.prediction for p in predictions]).cpu().numpy()\n",
    "\n",
    "        for product_id in product_ids:\n",
    "            product_data = self.val_df[self.val_df['product_id'] == str(product_id)]\n",
    "            if len(product_data) == 0:\n",
    "                print(f\"No data found for product {product_id}\")\n",
    "                continue\n",
    "\n",
    "            product_data = product_data.sort_values(\"week_date\")\n",
    "            true_values = product_data[self.target_column].values\n",
    "\n",
    "            # 예측값 매핑\n",
    "            pred_indices = product_data.index\n",
    "            if len(pred_indices) > len(predictions):\n",
    "                pred_indices = pred_indices[-len(predictions):]\n",
    "            product_predictions = predictions[:len(pred_indices)]\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(product_data['week_date'], true_values, color='blue', label='Actual')\n",
    "            plt.plot(product_data['week_date'].iloc[-len(product_predictions):], \n",
    "                    product_predictions, color='red', label='Predicted')\n",
    "            plt.title(f\"Product {product_id}: Actual vs Predicted Values\")\n",
    "            plt.xlabel(\"Week\")\n",
    "            plt.ylabel(\"Sales\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    def predict_next_week(self, product_id):\n",
    "        \"\"\"\n",
    "        특정 상품의 다음 주 판매량 예측\n",
    "        \"\"\"\n",
    "        product_data = self.df[self.df['product_id'] == str(product_id)]\n",
    "        if len(product_data) == 0:\n",
    "            print(f\"No data found for product {product_id}\")\n",
    "            return None\n",
    "\n",
    "        encoder_data = self.training.get_inference_data(\n",
    "            product_data,\n",
    "            predict_mode=True\n",
    "        )\n",
    "        \n",
    "        predictions = self.tft.predict(encoder_data)\n",
    "        next_week_prediction = predictions[0].cpu().numpy()\n",
    "\n",
    "        # 통계 계산\n",
    "        historical_stats = {\n",
    "            'mean': product_data[self.target_column].mean(),\n",
    "            'max': product_data[self.target_column].max(),\n",
    "            'min': product_data[self.target_column].min(),\n",
    "            'last': product_data[self.target_column].iloc[-1]\n",
    "        }\n",
    "\n",
    "        print(f\"\\nProduct {product_id} Statistics:\")\n",
    "        print(f\"  Historical Mean Sales: {historical_stats['mean']:.2f}\")\n",
    "        print(f\"  Historical Max Sales: {historical_stats['max']:.2f}\")\n",
    "        print(f\"  Historical Min Sales: {historical_stats['min']:.2f}\")\n",
    "        print(f\"  Last Week Sales: {historical_stats['last']:.2f}\")\n",
    "        print(f\"  Predicted Next Week Sales: {next_week_prediction[0]:.2f}\")\n",
    "\n",
    "        return next_week_prediction[0]\n",
    "\n",
    "# 사용 예시\n",
    "#if __name__ == \"__main__\":\n",
    "    # 필요한 변수 설정\n",
    "data_path = \"./fina_preprocessing_data.csv\"\n",
    "features = [ \n",
    "             'price',\n",
    "             'review_cnt',\n",
    "             'wish_cnt',\n",
    "             'sixMothRatio(puchase_cnt/review_cnt)',\n",
    "             'week_review_count',\n",
    "             'average_review_score',\n",
    "             'category1_encoded',\n",
    "             'category2_encoded',\n",
    "             'category3_encoded',\n",
    "             'rolling_mean_purchase',\n",
    "             'rolling_std_purchase',\n",
    "             'week_num',\n",
    "             'month',\n",
    "             'month_sin',\n",
    "             'month_cos',\n",
    "             'week_sin',\n",
    "             'week_cos']\n",
    "\n",
    "target_column = 'week_purchase_cnt'\n",
    "max_encoder_length = 24\n",
    "max_prediction_length = 1\n",
    "batch_size = 64\n",
    "\n",
    "# 모델 인스턴스 생성 및 학습\n",
    "tft_model = TFTModel(data_path, features, target_column, \n",
    "                    max_encoder_length, max_prediction_length, batch_size)\n",
    "tft_model.fit()\n",
    "\n",
    "# 평가 및 예측\n",
    "predictions = tft_model.evaluate()\n",
    "\n",
    "# 예시 상품들에 대한 예측 시각화\n",
    "sample_products = tft_model.df['product_id'].unique()[:3]\n",
    "print(sample_products)\n",
    "tft_model.plot_predictions(sample_products)\n",
    "\n",
    "# 특정 상품의 다음 주 예측\n",
    "tft_model.predict_next_week(sample_products[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "264b84de-5cdc-4efd-8be2-244c27607c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.5.0)\n",
      "Collecting torch\n",
      "  Downloading torch-2.5.1-cp310-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.20.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.5.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: pytorch-lightning in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.4.0)\n",
      "Requirement already satisfied: pytorch-forecasting in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.1.1)\n",
      "Collecting pytorch-forecasting\n",
      "  Downloading pytorch_forecasting-1.2.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/jenzennii/Library/Python/3.10/lib/python/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-lightning) (4.66.6)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-lightning) (6.0.2)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-lightning) (1.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-lightning) (23.1)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-lightning) (0.11.8)\n",
      "Requirement already satisfied: lightning<3.0.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-forecasting) (2.4.0)\n",
      "Requirement already satisfied: scipy<2.0,>=1.8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-forecasting) (1.14.1)\n",
      "Requirement already satisfied: pandas<3.0.0,>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-forecasting) (2.1.1)\n",
      "Requirement already satisfied: scikit-learn<2.0,>=1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-forecasting) (1.5.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.10.10)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (65.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn<2.0,>=1.2->pytorch-forecasting) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn<2.0,>=1.2->pytorch-forecasting) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.17.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=1.3.0->pytorch-forecasting) (1.16.0)\n",
      "Requirement already satisfied: idna>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.2.0)\n",
      "Downloading torch-2.5.1-cp310-none-macosx_11_0_arm64.whl (63.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.20.1-cp310-cp310-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.5.1-cp310-cp310-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytorch_forecasting-1.2.0-py3-none-any.whl (181 kB)\n",
      "Installing collected packages: torch, torchvision, torchaudio, pytorch-forecasting\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.0\n",
      "    Uninstalling torch-2.5.0:\n",
      "      Successfully uninstalled torch-2.5.0\n",
      "  Attempting uninstall: pytorch-forecasting\n",
      "    Found existing installation: pytorch-forecasting 1.1.1\n",
      "    Uninstalling pytorch-forecasting-1.1.1:\n",
      "      Successfully uninstalled pytorch-forecasting-1.1.1\n",
      "Successfully installed pytorch-forecasting-1.2.0 torch-2.5.1 torchaudio-2.5.1 torchvision-0.20.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade torch torchvision torchaudio pytorch-lightning pytorch-forecasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e2fb0ca0-ee8c-432b-8294-1267d09d4b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch\n",
      "  Downloading pytorch-1.0.2.tar.gz (689 bytes)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: pytorch\n",
      "  Building wheel for pytorch (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[6 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/c4/2xfj413907ng2mds_4tqrm8r0000gn/T/pip-install-2fttb2s9/pytorch_357cb588e0724029ab180095aadf7dd2/setup.py\", line 15, in <module>\n",
      "  \u001b[31m   \u001b[0m     raise Exception(message)\n",
      "  \u001b[31m   \u001b[0m Exception: You tried to install \"pytorch\". The package named for PyTorch is \"torch\"\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for pytorch\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for pytorch\n",
      "Failed to build pytorch\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (pytorch)\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7b9597ca-da7d-41d6-94de-060dfc3d8a7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.20.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/jenzennii/Library/Python/3.10/lib/python/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b4cddec5-97e8-444d-b7c2-c478d60377a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.5.1\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3-Clause\n",
      "Location: /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, sympy, typing-extensions\n",
      "Required-by: lightning, pytorch-forecasting, pytorch-lightning, sentence-transformers, torchaudio, torchmetrics, torchvision\n",
      "---\n",
      "Name: pytorch-lightning\n",
      "Version: 2.4.0\n",
      "Summary: PyTorch Lightning is the lightweight PyTorch wrapper for ML researchers. Scale your models. Write less boilerplate.\n",
      "Home-page: https://github.com/Lightning-AI/lightning\n",
      "Author: Lightning AI et al.\n",
      "Author-email: pytorch@lightning.ai\n",
      "License: Apache-2.0\n",
      "Location: /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages\n",
      "Requires: fsspec, lightning-utilities, packaging, PyYAML, torch, torchmetrics, tqdm, typing-extensions\n",
      "Required-by: lightning\n",
      "---\n",
      "Name: pytorch-forecasting\n",
      "Version: 1.2.0\n",
      "Summary: Forecasting timeseries with PyTorch - dataloaders, normalizers, metrics and models\n",
      "Home-page: \n",
      "Author: Jan Beitner\n",
      "Author-email: \n",
      "License: \n",
      "Location: /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages\n",
      "Requires: lightning, numpy, pandas, scikit-learn, scipy, torch\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show torch pytorch-lightning pytorch-forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b50385b-c17e-4903-a62c-8070f4f1ca2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2af31997-e805-43c4-94ad-6470e8435877",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pytorch_forecasting/data/timeseries.py:1282: UserWarning: Min encoder length and/or min_prediction_idx and/or min prediction length and/or lags are too large for 41 series/groups which therefore are not present in the dataset index. This means no predictions can be made for those series. First 10 removed groups: [{'__group_id__product_id': '82562973333'}, {'__group_id__product_id': '87226364197'}, {'__group_id__product_id': '87896780670'}, {'__group_id__product_id': '87896784213'}, {'__group_id__product_id': '88042074458'}, {'__group_id__product_id': '88042110947'}, {'__group_id__product_id': '88081241709'}, {'__group_id__product_id': '88098187751'}, {'__group_id__product_id': '88100622609'}, {'__group_id__product_id': '88113889505'}]\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pytorch_forecasting/data/timeseries.py:1282: UserWarning: Min encoder length and/or min_prediction_idx and/or min prediction length and/or lags are too large for 418 series/groups which therefore are not present in the dataset index. This means no predictions can be made for those series. First 10 removed groups: [{'__group_id__product_id': '10022619427'}, {'__group_id__product_id': '10095244123'}, {'__group_id__product_id': '10129739316'}, {'__group_id__product_id': '10182440260'}, {'__group_id__product_id': '10194331314'}, {'__group_id__product_id': '10276314469'}, {'__group_id__product_id': '10316027559'}, {'__group_id__product_id': '10583217322'}, {'__group_id__product_id': '10651348891'}, {'__group_id__product_id': '10790326479'}]\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py:143: UserWarning: In pytorch-forecasting models, on versions 1.1.X, the default optimizer defaults to 'adam', if pytorch_optimizer is not installed, otherwise it defaults to 'ranger' from pytorch_optimizer. From version 1.2.0, the default optimizer will be 'adam' regardless of whether pytorch_optimizer is installed, in order to minimize the number of dependencies in default parameter settings. Users who wish to ensure their code continues using 'ranger' as optimizer should ensure that pytorch_optimizer is installed, and set the optimizer parameter explicitly to 'ranger'.\n",
      "  super().__init__(loss=loss, logging_metrics=logging_metrics, **kwargs)\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TemporalFusionTransformer(\n",
      "  \t\"attention_head_size\":               1\n",
      "  \t\"categorical_groups\":                {}\n",
      "  \t\"causal_attention\":                  True\n",
      "  \t\"dataset_parameters\":                {'time_idx': 'time_idx', 'target': 'week_purchase_cnt', 'group_ids': ['product_id'], 'weight': None, 'max_encoder_length': 24, 'min_encoder_length': 12, 'min_prediction_idx': 0, 'min_prediction_length': 1, 'max_prediction_length': 1, 'static_categoricals': ['product_id'], 'static_reals': ['encoder_length'], 'time_varying_known_categoricals': [], 'time_varying_known_reals': ['time_idx', 'price', 'review_cnt', 'wish_cnt', 'sixMothRatio(puchase_cnt/review_cnt)', 'week_review_count', 'average_review_score', 'category1_encoded', 'category2_encoded', 'category3_encoded', 'rolling_mean_purchase', 'rolling_std_purchase', 'week_num', 'month', 'month_sin', 'month_cos', 'week_sin', 'week_cos'], 'time_varying_unknown_categoricals': [], 'time_varying_unknown_reals': ['week_purchase_cnt'], 'variable_groups': {}, 'constant_fill_strategy': {}, 'allow_missing_timesteps': False, 'lags': {}, 'add_relative_time_idx': False, 'add_target_scales': False, 'add_encoder_length': True, 'target_normalizer': GroupNormalizer(\n",
      "  \t\tmethod='standard',\n",
      "  \t\tgroups=['product_id'],\n",
      "  \t\tcenter=True,\n",
      "  \t\tscale_by_group=False,\n",
      "  \t\ttransformation=None,\n",
      "  \t\tmethod_kwargs={}\n",
      "  \t), 'categorical_encoders': {'__group_id__product_id': NaNLabelEncoder(add_nan=False, warn=True), 'product_id': NaNLabelEncoder(add_nan=False, warn=True)}, 'scalers': {'encoder_length': StandardScaler(), 'time_idx': StandardScaler(), 'price': StandardScaler(), 'review_cnt': StandardScaler(), 'wish_cnt': StandardScaler(), 'sixMothRatio(puchase_cnt/review_cnt)': StandardScaler(), 'week_review_count': StandardScaler(), 'average_review_score': StandardScaler(), 'category1_encoded': StandardScaler(), 'category2_encoded': StandardScaler(), 'category3_encoded': StandardScaler(), 'rolling_mean_purchase': StandardScaler(), 'rolling_std_purchase': StandardScaler(), 'week_num': StandardScaler(), 'month': StandardScaler(), 'month_sin': StandardScaler(), 'month_cos': StandardScaler(), 'week_sin': StandardScaler(), 'week_cos': StandardScaler()}, 'randomize_length': None, 'predict_mode': False}\n",
      "  \t\"dropout\":                           0.1\n",
      "  \t\"embedding_labels\":                  {'product_id': {'10022619427': 0, '10095244123': 1, '10129739316': 2, '10182440260': 3, '10194331314': 4, '10276314469': 5, '10316027559': 6, '10583217322': 7, '10651348891': 8, '10790326479': 9, '10839131148': 10, '10857366633': 11, '10990197556': 12, '11076937403': 13, '11195202991': 14, '11199076229': 15, '11218721969': 16, '11239882600': 17, '11421983102': 18, '11545229477': 19, '11598092065': 20, '11659574428': 21, '11779789592': 22, '11831091502': 23, '11913737888': 24, '11976178121': 25, '11985210471': 26, '12099494038': 27, '12261313683': 28, '12402425757': 29, '12406570359': 30, '12549294732': 31, '12664919543': 32, '12823927460': 33, '13147435734': 34, '13154622884': 35, '13381025014': 36, '13399566549': 37, '6356018199': 38, '6485255842': 39, '6617143657': 40, '6766327742': 41, '7506929597': 42, '7781174622': 43, '80027880899': 44, '80036527278': 45, '80070124325': 46, '80137720714': 47, '80150432791': 48, '80151600598': 49, '80179015780': 50, '80260009881': 51, '80437478266': 52, '80447378951': 53, '80466364794': 54, '80471449409': 55, '80538088436': 56, '80585484638': 57, '80771696270': 58, '80909194289': 59, '80916609335': 60, '80998837833': 61, '81011449761': 62, '81022718754': 63, '81029307872': 64, '81037586677': 65, '81044807932': 66, '81072587300': 67, '81076354187': 68, '81162882669': 69, '81165039187': 70, '81182199627': 71, '81251971550': 72, '81307567948': 73, '81376726447': 74, '81432917824': 75, '81432925718': 76, '81490308013': 77, '81500447696': 78, '81592586014': 79, '81683115256': 80, '81743617172': 81, '81743662525': 82, '81769963134': 83, '81817765030': 84, '81883717746': 85, '81898265092': 86, '81944633724': 87, '81950814342': 88, '81977652281': 89, '81999320307': 90, '82035499213': 91, '82048947651': 92, '82057321861': 93, '82075021040': 94, '82084083327': 95, '82094228515': 96, '82100081230': 97, '82109132933': 98, '82114593556': 99, '82116389223': 100, '82141150395': 101, '82154885060': 102, '82155387209': 103, '82161091686': 104, '82167582057': 105, '82177741646': 106, '8218195764': 107, '82182472219': 108, '82194575327': 109, '82196153989': 110, '82198829607': 111, '82201860750': 112, '82203873495': 113, '82213682976': 114, '82224112824': 115, '82224200414': 116, '82235144540': 117, '82237266787': 118, '82239755179': 119, '82243364662': 120, '82244034272': 121, '82245440734': 122, '82250264065': 123, '82256221887': 124, '82261431626': 125, '82271542438': 126, '82278485824': 127, '82280680602': 128, '82288244906': 129, '82289916004': 130, '82293068454': 131, '82298266676': 132, '82308116319': 133, '82330465832': 134, '82343766518': 135, '82355482914': 136, '82366359065': 137, '82371516080': 138, '82374636770': 139, '82375566008': 140, '82386649071': 141, '82396216124': 142, '82409708164': 143, '82413203779': 144, '82423604772': 145, '82431335523': 146, '82439360120': 147, '82442958695': 148, '82450880093': 149, '82473335098': 150, '82484077890': 151, '82492189459': 152, '82493839762': 153, '82493840170': 154, '82498044964': 155, '82506716986': 156, '82512029192': 157, '82518325576': 158, '82518358819': 159, '82554079402': 160, '82562973333': 161, '82579351820': 162, '82584169242': 163, '82612392967': 164, '82617930980': 165, '82629272072': 166, '82644613191': 167, '82658175350': 168, '82663901180': 169, '82697228937': 170, '82725368080': 171, '82731789250': 172, '82748436013': 173, '82797428689': 174, '82803508510': 175, '82839137204': 176, '82855381541': 177, '82888223999': 178, '82904703004': 179, '82909794326': 180, '82920847039': 181, '82923956263': 182, '82938287668': 183, '82964195824': 184, '82978885242': 185, '82987858627': 186, '83018697152': 187, '83021652171': 188, '83026221734': 189, '83027433275': 190, '83032577198': 191, '83046474193': 192, '83058672505': 193, '83060973890': 194, '83124441105': 195, '83145373731': 196, '83148737984': 197, '83148914744': 198, '83154350949': 199, '83165781839': 200, '83168990735': 201, '83176583239': 202, '83185458721': 203, '83195131633': 204, '83196162009': 205, '83227652369': 206, '83242599500': 207, '83265103095': 208, '83339174174': 209, '83342948311': 210, '83353405499': 211, '83355904195': 212, '83357689027': 213, '83379992650': 214, '83396511185': 215, '83405521509': 216, '83412461028': 217, '83423045215': 218, '83439829432': 219, '83441947093': 220, '83442851433': 221, '83450413204': 222, '83452559067': 223, '83459206628': 224, '83469823485': 225, '83496016535': 226, '83544700181': 227, '83565897543': 228, '83582241229': 229, '83586891935': 230, '83588420791': 231, '83588503201': 232, '83595717401': 233, '83715863755': 234, '83722394844': 235, '83729421264': 236, '83817222587': 237, '83835508396': 238, '83844563428': 239, '83886328887': 240, '83890388520': 241, '83892959540': 242, '83916701408': 243, '83937248946': 244, '83944786359': 245, '83945044942': 246, '83956059687': 247, '83969946618': 248, '83988837837': 249, '83989019167': 250, '84051723307': 251, '84195356355': 252, '84223183690': 253, '84240189171': 254, '84253456764': 255, '84266140607': 256, '84338752005': 257, '84345558721': 258, '84366233772': 259, '84371446205': 260, '84417974537': 261, '84482901377': 262, '84486897225': 263, '84575983577': 264, '84590609622': 265, '84593454966': 266, '84595711640': 267, '84630569977': 268, '84651414760': 269, '84684925054': 270, '84739735201': 271, '84739899624': 272, '84759655602': 273, '84773371122': 274, '84852924321': 275, '84861658923': 276, '84878420163': 277, '84878502957': 278, '84920211651': 279, '84938641455': 280, '85008754353': 281, '85022115898': 282, '85050087425': 283, '85080929940': 284, '85113603549': 285, '85141197525': 286, '85219719569': 287, '85243095782': 288, '85338623974': 289, '85394939977': 290, '85465017384': 291, '85489125061': 292, '85556153151': 293, '85577317693': 294, '85578911636': 295, '85602479174': 296, '85603598515': 297, '85622876945': 298, '85635072114': 299, '85637826843': 300, '85674849480': 301, '85676427375': 302, '85747878342': 303, '85750841326': 304, '85765526144': 305, '85786543730': 306, '85827942083': 307, '85829941612': 308, '85903271022': 309, '85946192170': 310, '8597689401': 311, '85994962324': 312, '86001457602': 313, '86003052939': 314, '86011600522': 315, '86015286906': 316, '86074262188': 317, '86080579187': 318, '86168008312': 319, '86196224969': 320, '86238161590': 321, '86238199436': 322, '86253563442': 323, '86258398286': 324, '86307626335': 325, '86316244276': 326, '86340219528': 327, '86361182801': 328, '86380103731': 329, '86512422117': 330, '86512643840': 331, '86520272366': 332, '86543062469': 333, '86552777853': 334, '86564530041': 335, '86575042896': 336, '86596962625': 337, '86615650299': 338, '86634985389': 339, '86644487746': 340, '86677200634': 341, '86695180428': 342, '86716255500': 343, '86718087741': 344, '86734323610': 345, '86744054774': 346, '86750799549': 347, '86766394229': 348, '86768411788': 349, '86779626334': 350, '8678210047': 351, '86865818817': 352, '86892789310': 353, '86931391148': 354, '86947868795': 355, '86984516623': 356, '86985198344': 357, '87002755797': 358, '87034617231': 359, '87044616207': 360, '87077078211': 361, '87134721472': 362, '87149885404': 363, '87154257622': 364, '87156542226': 365, '87171220291': 366, '87194315468': 367, '87207387394': 368, '87226364197': 369, '87233647417': 370, '87256966784': 371, '87297137510': 372, '87300027307': 373, '87354688430': 374, '87416266422': 375, '87434901086': 376, '8747453599': 377, '87477209865': 378, '87528396162': 379, '87603328228': 380, '87604505038': 381, '87612495327': 382, '87616961155': 383, '87631394749': 384, '87642859084': 385, '87647012257': 386, '87664970794': 387, '87703272849': 388, '87730178392': 389, '87730829177': 390, '87737396955': 391, '87817018176': 392, '87844396267': 393, '87896780670': 394, '87896784213': 395, '87921020002': 396, '87937248809': 397, '87952389253': 398, '87958278370': 399, '87962457602': 400, '87965476173': 401, '87988876731': 402, '87991745133': 403, '88012479244': 404, '88038712401': 405, '88042074458': 406, '88042110947': 407, '88081241709': 408, '8808295228': 409, '88098187751': 410, '88100622609': 411, '88113889505': 412, '88117899204': 413, '88119947030': 414, '88184129427': 415, '88206700121': 416, '88208430684': 417, '88218200275': 418, '88222671639': 419, '88242709418': 420, '88283419814': 421, '88284816214': 422, '88291977922': 423, '88303490649': 424, '88321577019': 425, '88334988204': 426, '88347463796': 427, '88351942304': 428, '88356536951': 429, '88378913358': 430, '88380500239': 431, '88388533454': 432, '88388649996': 433, '88388890335': 434, '88401777165': 435, '88411622436': 436, '88415191727': 437, '88415318369': 438, '88415339810': 439, '88438439266': 440, '88453896846': 441, '88459191933': 442, '88473568229': 443, '8866795532': 444, '8974847230': 445, '9061388274': 446, '9270155631': 447, '9508439307': 448, '9555749627': 449, '9645322788': 450, '9694820143': 451, '9879202686': 452}}\n",
      "  \t\"embedding_paddings\":                []\n",
      "  \t\"embedding_sizes\":                   {'product_id': (453, 49)}\n",
      "  \t\"hidden_continuous_size\":            8\n",
      "  \t\"hidden_continuous_sizes\":           {}\n",
      "  \t\"hidden_size\":                       16\n",
      "  \t\"learning_rate\":                     0.03\n",
      "  \t\"log_gradient_flow\":                 False\n",
      "  \t\"log_interval\":                      -1\n",
      "  \t\"log_val_interval\":                  -1\n",
      "  \t\"lstm_layers\":                       1\n",
      "  \t\"max_encoder_length\":                24\n",
      "  \t\"monotone_constaints\":               {}\n",
      "  \t\"optimizer\":                         adam\n",
      "  \t\"optimizer_params\":                  None\n",
      "  \t\"output_size\":                       1\n",
      "  \t\"output_transformer\":                GroupNormalizer(\n",
      "  \t\tmethod='standard',\n",
      "  \t\tgroups=['product_id'],\n",
      "  \t\tcenter=True,\n",
      "  \t\tscale_by_group=False,\n",
      "  \t\ttransformation=None,\n",
      "  \t\tmethod_kwargs={}\n",
      "  \t)\n",
      "  \t\"reduce_on_plateau_min_lr\":          1e-05\n",
      "  \t\"reduce_on_plateau_patience\":        1000\n",
      "  \t\"reduce_on_plateau_reduction\":       2.0\n",
      "  \t\"share_single_variable_networks\":    False\n",
      "  \t\"static_categoricals\":               ['product_id']\n",
      "  \t\"static_reals\":                      ['encoder_length']\n",
      "  \t\"time_varying_categoricals_decoder\": []\n",
      "  \t\"time_varying_categoricals_encoder\": []\n",
      "  \t\"time_varying_reals_decoder\":        ['time_idx', 'price', 'review_cnt', 'wish_cnt', 'sixMothRatio(puchase_cnt/review_cnt)', 'week_review_count', 'average_review_score', 'category1_encoded', 'category2_encoded', 'category3_encoded', 'rolling_mean_purchase', 'rolling_std_purchase', 'week_num', 'month', 'month_sin', 'month_cos', 'week_sin', 'week_cos']\n",
      "  \t\"time_varying_reals_encoder\":        ['time_idx', 'price', 'review_cnt', 'wish_cnt', 'sixMothRatio(puchase_cnt/review_cnt)', 'week_review_count', 'average_review_score', 'category1_encoded', 'category2_encoded', 'category3_encoded', 'rolling_mean_purchase', 'rolling_std_purchase', 'week_num', 'month', 'month_sin', 'month_cos', 'week_sin', 'week_cos', 'week_purchase_cnt']\n",
      "  \t\"weight_decay\":                      0.0\n",
      "  \t\"x_categoricals\":                    ['product_id']\n",
      "  \t\"x_reals\":                           ['encoder_length', 'time_idx', 'price', 'review_cnt', 'wish_cnt', 'sixMothRatio(puchase_cnt/review_cnt)', 'week_review_count', 'average_review_score', 'category1_encoded', 'category2_encoded', 'category3_encoded', 'rolling_mean_purchase', 'rolling_std_purchase', 'week_num', 'month', 'month_sin', 'month_cos', 'week_sin', 'week_cos', 'week_purchase_cnt']\n",
      "  (loss): RMSE()\n",
      "  (logging_metrics): ModuleList(\n",
      "    (0): SMAPE()\n",
      "    (1): MAE()\n",
      "    (2): RMSE()\n",
      "    (3): MAPE()\n",
      "  )\n",
      "  (input_embeddings): MultiEmbedding(\n",
      "    (embeddings): ModuleDict(\n",
      "      (product_id): Embedding(453, 16)\n",
      "    )\n",
      "  )\n",
      "  (prescalers): ModuleDict(\n",
      "    (encoder_length): Linear(in_features=1, out_features=8, bias=True)\n",
      "    (time_idx): Linear(in_features=1, out_features=8, bias=True)\n",
      "    (price): Linear(in_features=1, out_features=8, bias=True)\n",
      "    (review_cnt): Linear(in_features=1, out_features=8, bias=True)\n",
      "    (wish_cnt): Linear(in_features=1, out_features=8, bias=True)\n",
      "    (sixMothRatio(puchase_cnt/review_cnt)): Linear(in_features=1, out_features=8, bias=True)\n",
      "    (week_review_count): Linear(in_features=1, out_features=8, bias=True)\n",
      "    (average_review_score): Linear(in_features=1, out_features=8, bias=True)\n",
      "    (category1_encoded): Linear(in_features=1, out_features=8, bias=True)\n",
      "    (category2_encoded): Linear(in_features=1, out_features=8, bias=True)\n",
      "    (category3_encoded): Linear(in_features=1, out_features=8, bias=True)\n",
      "    (rolling_mean_purchase): Linear(in_features=1, out_features=8, bias=True)\n",
      "    (rolling_std_purchase): Linear(in_features=1, out_features=8, bias=True)\n",
      "    (week_num): Linear(in_features=1, out_features=8, bias=True)\n",
      "    (month): Linear(in_features=1, out_features=8, bias=True)\n",
      "    (month_sin): Linear(in_features=1, out_features=8, bias=True)\n",
      "    (month_cos): Linear(in_features=1, out_features=8, bias=True)\n",
      "    (week_sin): Linear(in_features=1, out_features=8, bias=True)\n",
      "    (week_cos): Linear(in_features=1, out_features=8, bias=True)\n",
      "    (week_purchase_cnt): Linear(in_features=1, out_features=8, bias=True)\n",
      "  )\n",
      "  (static_variable_selection): VariableSelectionNetwork(\n",
      "    (flattened_grn): GatedResidualNetwork(\n",
      "      (resample_norm): ResampleNorm(\n",
      "        (resample): TimeDistributedInterpolation()\n",
      "        (gate): Sigmoid()\n",
      "        (norm): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (fc1): Linear(in_features=24, out_features=2, bias=True)\n",
      "      (elu): ELU(alpha=1.0)\n",
      "      (fc2): Linear(in_features=2, out_features=2, bias=True)\n",
      "      (gate_norm): GateAddNorm(\n",
      "        (glu): GatedLinearUnit(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (fc): Linear(in_features=2, out_features=4, bias=True)\n",
      "        )\n",
      "        (add_norm): AddNorm(\n",
      "          (norm): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (single_variable_grns): ModuleDict(\n",
      "      (product_id): ResampleNorm(\n",
      "        (gate): Sigmoid()\n",
      "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (encoder_length): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (prescalers): ModuleDict(\n",
      "      (encoder_length): Linear(in_features=1, out_features=8, bias=True)\n",
      "    )\n",
      "    (softmax): Softmax(dim=-1)\n",
      "  )\n",
      "  (encoder_variable_selection): VariableSelectionNetwork(\n",
      "    (flattened_grn): GatedResidualNetwork(\n",
      "      (resample_norm): ResampleNorm(\n",
      "        (resample): TimeDistributedInterpolation()\n",
      "        (gate): Sigmoid()\n",
      "        (norm): LayerNorm((19,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (fc1): Linear(in_features=152, out_features=16, bias=True)\n",
      "      (elu): ELU(alpha=1.0)\n",
      "      (context): Linear(in_features=16, out_features=16, bias=False)\n",
      "      (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "      (gate_norm): GateAddNorm(\n",
      "        (glu): GatedLinearUnit(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (fc): Linear(in_features=16, out_features=38, bias=True)\n",
      "        )\n",
      "        (add_norm): AddNorm(\n",
      "          (norm): LayerNorm((19,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (single_variable_grns): ModuleDict(\n",
      "      (time_idx): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (price): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (review_cnt): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (wish_cnt): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (sixMothRatio(puchase_cnt/review_cnt)): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (week_review_count): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (average_review_score): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (category1_encoded): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (category2_encoded): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (category3_encoded): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (rolling_mean_purchase): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (rolling_std_purchase): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (week_num): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (month): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (month_sin): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (month_cos): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (week_sin): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (week_cos): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (week_purchase_cnt): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (prescalers): ModuleDict(\n",
      "      (time_idx): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (price): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (review_cnt): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (wish_cnt): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (sixMothRatio(puchase_cnt/review_cnt)): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (week_review_count): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (average_review_score): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (category1_encoded): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (category2_encoded): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (category3_encoded): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (rolling_mean_purchase): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (rolling_std_purchase): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (week_num): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (month): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (month_sin): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (month_cos): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (week_sin): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (week_cos): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (week_purchase_cnt): Linear(in_features=1, out_features=8, bias=True)\n",
      "    )\n",
      "    (softmax): Softmax(dim=-1)\n",
      "  )\n",
      "  (decoder_variable_selection): VariableSelectionNetwork(\n",
      "    (flattened_grn): GatedResidualNetwork(\n",
      "      (resample_norm): ResampleNorm(\n",
      "        (resample): TimeDistributedInterpolation()\n",
      "        (gate): Sigmoid()\n",
      "        (norm): LayerNorm((18,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (fc1): Linear(in_features=144, out_features=16, bias=True)\n",
      "      (elu): ELU(alpha=1.0)\n",
      "      (context): Linear(in_features=16, out_features=16, bias=False)\n",
      "      (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "      (gate_norm): GateAddNorm(\n",
      "        (glu): GatedLinearUnit(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (fc): Linear(in_features=16, out_features=36, bias=True)\n",
      "        )\n",
      "        (add_norm): AddNorm(\n",
      "          (norm): LayerNorm((18,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (single_variable_grns): ModuleDict(\n",
      "      (time_idx): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (price): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (review_cnt): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (wish_cnt): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (sixMothRatio(puchase_cnt/review_cnt)): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (week_review_count): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (average_review_score): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (category1_encoded): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (category2_encoded): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (category3_encoded): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (rolling_mean_purchase): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (rolling_std_purchase): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (week_num): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (month): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (month_sin): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (month_cos): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (week_sin): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (week_cos): GatedResidualNetwork(\n",
      "        (resample_norm): ResampleNorm(\n",
      "          (resample): TimeDistributedInterpolation()\n",
      "          (gate): Sigmoid()\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (fc1): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (elu): ELU(alpha=1.0)\n",
      "        (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (gate_norm): GateAddNorm(\n",
      "          (glu): GatedLinearUnit(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (fc): Linear(in_features=8, out_features=32, bias=True)\n",
      "          )\n",
      "          (add_norm): AddNorm(\n",
      "            (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (prescalers): ModuleDict(\n",
      "      (time_idx): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (price): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (review_cnt): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (wish_cnt): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (sixMothRatio(puchase_cnt/review_cnt)): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (week_review_count): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (average_review_score): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (category1_encoded): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (category2_encoded): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (category3_encoded): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (rolling_mean_purchase): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (rolling_std_purchase): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (week_num): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (month): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (month_sin): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (month_cos): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (week_sin): Linear(in_features=1, out_features=8, bias=True)\n",
      "      (week_cos): Linear(in_features=1, out_features=8, bias=True)\n",
      "    )\n",
      "    (softmax): Softmax(dim=-1)\n",
      "  )\n",
      "  (static_context_variable_selection): GatedResidualNetwork(\n",
      "    (fc1): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "    (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (gate_norm): GateAddNorm(\n",
      "      (glu): GatedLinearUnit(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (fc): Linear(in_features=16, out_features=32, bias=True)\n",
      "      )\n",
      "      (add_norm): AddNorm(\n",
      "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (static_context_initial_hidden_lstm): GatedResidualNetwork(\n",
      "    (fc1): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "    (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (gate_norm): GateAddNorm(\n",
      "      (glu): GatedLinearUnit(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (fc): Linear(in_features=16, out_features=32, bias=True)\n",
      "      )\n",
      "      (add_norm): AddNorm(\n",
      "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (static_context_initial_cell_lstm): GatedResidualNetwork(\n",
      "    (fc1): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "    (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (gate_norm): GateAddNorm(\n",
      "      (glu): GatedLinearUnit(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (fc): Linear(in_features=16, out_features=32, bias=True)\n",
      "      )\n",
      "      (add_norm): AddNorm(\n",
      "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (static_context_enrichment): GatedResidualNetwork(\n",
      "    (fc1): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "    (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (gate_norm): GateAddNorm(\n",
      "      (glu): GatedLinearUnit(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (fc): Linear(in_features=16, out_features=32, bias=True)\n",
      "      )\n",
      "      (add_norm): AddNorm(\n",
      "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lstm_encoder): LSTM(16, 16, batch_first=True)\n",
      "  (lstm_decoder): LSTM(16, 16, batch_first=True)\n",
      "  (post_lstm_gate_encoder): GatedLinearUnit(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (fc): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (post_lstm_gate_decoder): GatedLinearUnit(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (fc): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (post_lstm_add_norm_encoder): AddNorm(\n",
      "    (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (post_lstm_add_norm_decoder): AddNorm(\n",
      "    (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (static_enrichment): GatedResidualNetwork(\n",
      "    (fc1): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "    (context): Linear(in_features=16, out_features=16, bias=False)\n",
      "    (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (gate_norm): GateAddNorm(\n",
      "      (glu): GatedLinearUnit(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (fc): Linear(in_features=16, out_features=32, bias=True)\n",
      "      )\n",
      "      (add_norm): AddNorm(\n",
      "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (multihead_attn): InterpretableMultiHeadAttention(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (v_layer): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (q_layers): ModuleList(\n",
      "      (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "    )\n",
      "    (k_layers): ModuleList(\n",
      "      (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "    )\n",
      "    (attention): ScaledDotProductAttention(\n",
      "      (softmax): Softmax(dim=2)\n",
      "    )\n",
      "    (w_h): Linear(in_features=16, out_features=16, bias=False)\n",
      "  )\n",
      "  (post_attn_gate_norm): GateAddNorm(\n",
      "    (glu): GatedLinearUnit(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (fc): Linear(in_features=16, out_features=32, bias=True)\n",
      "    )\n",
      "    (add_norm): AddNorm(\n",
      "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (pos_wise_ff): GatedResidualNetwork(\n",
      "    (fc1): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "    (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (gate_norm): GateAddNorm(\n",
      "      (glu): GatedLinearUnit(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (fc): Linear(in_features=16, out_features=32, bias=True)\n",
      "      )\n",
      "      (add_norm): AddNorm(\n",
      "        (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_output_gate_norm): GateAddNorm(\n",
      "    (glu): GatedLinearUnit(\n",
      "      (fc): Linear(in_features=16, out_features=32, bias=True)\n",
      "    )\n",
      "    (add_norm): AddNorm(\n",
      "      (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `TemporalFusionTransformer`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 267\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# 모델 인스턴스 생성 및 학습\u001b[39;00m\n\u001b[1;32m    265\u001b[0m tft_model \u001b[38;5;241m=\u001b[39m TFTModel(data_path, features, target_column, \n\u001b[1;32m    266\u001b[0m                     max_encoder_length, max_prediction_length, batch_size)\n\u001b[0;32m--> 267\u001b[0m \u001b[43mtft_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# # 평가 및 예측\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# predictions = tft_model.evaluate()\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# # 특정 상품의 다음 주 예측\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# tft_model.predict_next_week(sample_products[0])\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[50], line 131\u001b[0m, in \u001b[0;36mTFTModel.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[1;32m    130\u001b[0m pl\u001b[38;5;241m.\u001b[39mseed_everything(\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# 체크포인트 저장\u001b[39;00m\n\u001b[1;32m    138\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:532\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    500\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    504\u001b[0m     ckpt_path: Optional[_PATH] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    505\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Runs the full optimization routine.\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \n\u001b[1;32m    508\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    530\u001b[0m \n\u001b[1;32m    531\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 532\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_unwrap_optimized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    534\u001b[0m     _verify_strategy_supports_compile(model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pytorch_lightning/utilities/compile.py:111\u001b[0m, in \u001b[0;36m_maybe_unwrap_optimized\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m    110\u001b[0m _check_mixed_imports(model)\n\u001b[0;32m--> 111\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: `model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `TemporalFusionTransformer`"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import RMSE\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class TFTModel:\n",
    "    def __init__(self, data_path, features, target_column, max_encoder_length, max_prediction_length, batch_size, checkpoint_dir=\"./checkpoints\", model_save_dir=\"./weights\"):\n",
    "        # 데이터 로드\n",
    "        self.df = pd.read_csv(data_path)\n",
    "\n",
    "        # 데이터 전처리\n",
    "        self.features = features\n",
    "        self.target_column = target_column\n",
    "        self.max_encoder_length = max_encoder_length\n",
    "        self.max_prediction_length = max_prediction_length\n",
    "        self.batch_size = batch_size\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.model_save_dir = model_save_dir\n",
    "\n",
    "        # product_id 처리\n",
    "        self.df['product_id'] = self.df['product_id'].astype(str)  # product_id를 문자열로 변환\n",
    "        \n",
    "        # 날짜 처리\n",
    "        self.df['week_date'] = pd.to_datetime(self.df['week_date'])\n",
    "        self.df = self.df.sort_values(\"week_date\").reset_index(drop=True)\n",
    "        self.df[\"time_idx\"] = (self.df[\"week_date\"] - self.df[\"week_date\"].min()).dt.days // 7\n",
    "\n",
    "        # 각 product_id별로 데이터를 훈련/검증 세트로 분리\n",
    "        train_list = []\n",
    "        val_list = []\n",
    "\n",
    "        for product_id, group in self.df.groupby('product_id'):\n",
    "            # group은 각 product_id에 해당하는 데이터\n",
    "            group_train, group_val = train_test_split(\n",
    "                group, \n",
    "                test_size=0.2, \n",
    "                shuffle=False  # 시계열 데이터에서는 시간 순서를 유지\n",
    "            )\n",
    "            train_list.append(group_train)\n",
    "            val_list.append(group_val)\n",
    "\n",
    "        # 훈련/검증 데이터를 병합\n",
    "        self.train_df = pd.concat(train_list).reset_index(drop=True)\n",
    "        self.val_df = pd.concat(val_list).reset_index(drop=True)\n",
    "\n",
    "        # # 디버그: 카테고리 정보 출력\n",
    "        # print(\"Unique product_ids in training:\")\n",
    "        # print(self.train_df['product_id'].unique())\n",
    "        # print(\"\\nUnique product_ids in validation:\")\n",
    "        # print(self.val_df['product_id'].unique())\n",
    "\n",
    "        # # 전체 데이터셋의 고유 product_id\n",
    "        # all_product_ids = self.df['product_id'].unique()\n",
    "        # print(f\"\\nTotal unique product_ids: {len(all_product_ids)}\")\n",
    "        \n",
    "        # TimeSeriesDataSet 생성\n",
    "        self.training = TimeSeriesDataSet(\n",
    "            self.train_df[lambda x: x.time_idx <= self.train_df['time_idx'].max() - max_prediction_length],\n",
    "            time_idx=\"time_idx\",\n",
    "            target=self.target_column,\n",
    "            group_ids=[\"product_id\"],\n",
    "            min_encoder_length=max_encoder_length // 2,\n",
    "            max_encoder_length=max_encoder_length,\n",
    "            max_prediction_length=max_prediction_length,\n",
    "            static_categoricals=[\"product_id\"],\n",
    "            time_varying_known_reals=[\"time_idx\"] + features,\n",
    "            time_varying_unknown_reals=[target_column],\n",
    "            target_normalizer=GroupNormalizer(groups=[\"product_id\"]),\n",
    "            # add_nan=True  # 새로운 카테고리 처리\n",
    "        )\n",
    "\n",
    "        # validation 데이터셋 생성\n",
    "        self.validation = TimeSeriesDataSet.from_dataset(\n",
    "            self.training,\n",
    "            self.val_df,\n",
    "            predict=True,\n",
    "            stop_randomization=True\n",
    "        )\n",
    "\n",
    "        # 데이터 로더 준비\n",
    "        self.train_dataloader = self.training.to_dataloader(\n",
    "            train=True, \n",
    "            batch_size=batch_size, \n",
    "            num_workers=0\n",
    "        )\n",
    "        self.val_dataloader = self.validation.to_dataloader(\n",
    "            train=False, \n",
    "            batch_size=batch_size, \n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "        # 모델 정의\n",
    "        self.tft = TemporalFusionTransformer.from_dataset(\n",
    "            self.training,\n",
    "            learning_rate=0.03,\n",
    "            hidden_size=16,\n",
    "            attention_head_size=1,\n",
    "            dropout=0.1,\n",
    "            hidden_continuous_size=8,\n",
    "            output_size=1,\n",
    "            loss=RMSE(),\n",
    "        )\n",
    "\n",
    "        # Trainer 설정\n",
    "        self.trainer = pl.Trainer(\n",
    "            max_epochs=10,\n",
    "            devices=1,\n",
    "            accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "            gradient_clip_val=0.1,\n",
    "            # logger=pl.loggers.TensorBoardLogger('tb_logs'),\n",
    "            # callbacks=[pl.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)]\n",
    "        )\n",
    "        print(self.tft)\n",
    "\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        학습을 진행하는 함수\n",
    "        \"\"\"\n",
    "        # 모델 학습\n",
    "        pl.seed_everything(42)\n",
    "        self.trainer.fit(\n",
    "            model=self.tft,\n",
    "            train_dataloaders=self.train_dataloader,\n",
    "            val_dataloaders=self.val_dataloader\n",
    "        )\n",
    "\n",
    "        # 체크포인트 저장\n",
    "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "        checkpoint_path = os.path.join(self.checkpoint_dir, \"tft_model_checkpoint.ckpt\")\n",
    "        self.trainer.save_checkpoint(checkpoint_path)\n",
    "\n",
    "        # 모델 가중치 저장\n",
    "        os.makedirs(self.model_save_dir, exist_ok=True)\n",
    "        model_save_path = os.path.join(self.model_save_dir, \"tft_model_weights.pth\")\n",
    "        torch.save(self.tft.state_dict(), model_save_path)\n",
    "\n",
    "        print(\"모델 학습 완료 및 저장.\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        validation 데이터를 사용하여 모델 성능 평가\n",
    "        \"\"\"\n",
    "        predictions = self.trainer.predict(self.tft, self.val_dataloader)\n",
    "        predictions = torch.cat([p.prediction for p in predictions]).cpu().numpy()\n",
    "\n",
    "        # 실제 값과 예측값 비교\n",
    "        actuals = torch.cat([self.tft.to_network_output(batch)[0] for batch in iter(self.val_dataloader)]).cpu().numpy()\n",
    "        \n",
    "        mse = mean_squared_error(actuals, predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(actuals, predictions)\n",
    "        \n",
    "        print(f\"Validation RMSE: {rmse:.4f}, R2: {r2:.4f}\")\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def plot_predictions(self, product_ids):\n",
    "        \"\"\"\n",
    "        각 상품별 예측 결과를 실제 값과 함께 시각화\n",
    "        \"\"\"\n",
    "        predictions = self.trainer.predict(self.tft, self.val_dataloader)\n",
    "        predictions = torch.cat([p.prediction for p in predictions]).cpu().numpy()\n",
    "\n",
    "        for product_id in product_ids:\n",
    "            product_data = self.val_df[self.val_df['product_id'] == product_id]\n",
    "            if len(product_data) == 0:\n",
    "                print(f\"No data found for product {product_id}\")\n",
    "                continue\n",
    "\n",
    "            product_data = product_data.sort_values(\"week_date\")\n",
    "            true_values = product_data[self.target_column].values\n",
    "\n",
    "            # 예측값 매핑\n",
    "            pred_indices = product_data.index\n",
    "            if len(pred_indices) > len(predictions):\n",
    "                pred_indices = pred_indices[-len(predictions):]\n",
    "            product_predictions = predictions[:len(pred_indices)]\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(product_data['week_date'], true_values, color='blue', label='Actual')\n",
    "            plt.plot(product_data['week_date'].iloc[-len(product_predictions):], \n",
    "                    product_predictions, color='red', label='Predicted')\n",
    "            plt.title(f\"Product {product_id}: Actual vs Predicted Values\")\n",
    "            plt.xlabel(\"Week\")\n",
    "            plt.ylabel(\"Sales\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    def predict_next_week(self, product_id):\n",
    "        \"\"\"\n",
    "        특정 상품의 다음 주 판매량 예측\n",
    "        \"\"\"\n",
    "        product_data = self.df[self.df['product_id'] == product_id]\n",
    "        if len(product_data) == 0:\n",
    "            print(f\"No data found for product {product_id}\")\n",
    "            return None\n",
    "\n",
    "        encoder_data = self.training.get_inference_data(\n",
    "            product_data,\n",
    "            predict_mode=True\n",
    "        )\n",
    "        \n",
    "        predictions = self.tft.predict(encoder_data)\n",
    "        next_week_prediction = predictions[0].cpu().numpy()\n",
    "\n",
    "        # 통계 계산\n",
    "        historical_stats = {\n",
    "            'mean': product_data[self.target_column].mean(),\n",
    "            'max': product_data[self.target_column].max(),\n",
    "            'min': product_data[self.target_column].min(),\n",
    "            'last': product_data[self.target_column].iloc[-1]\n",
    "        }\n",
    "\n",
    "        print(f\"\\nProduct {product_id} Statistics:\")\n",
    "        print(f\"  Historical Mean Sales: {historical_stats['mean']:.2f}\")\n",
    "        print(f\"  Historical Max Sales: {historical_stats['max']:.2f}\")\n",
    "        print(f\"  Historical Min Sales: {historical_stats['min']:.2f}\")\n",
    "        print(f\"  Last Week Sales: {historical_stats['last']:.2f}\")\n",
    "        print(f\"  Predicted Next Week Sales: {next_week_prediction[0]:.2f}\")\n",
    "\n",
    "        return next_week_prediction[0]\n",
    "\n",
    "# 사용 예시\n",
    "#if __name__ == \"__main__\":\n",
    "    # 필요한 변수 설정\n",
    "data_path = \"./fina_preprocessing_data.csv\"\n",
    "features = [ \n",
    "             'price',\n",
    "             'review_cnt',\n",
    "             'wish_cnt',\n",
    "             'sixMothRatio(puchase_cnt/review_cnt)',\n",
    "             'week_review_count',\n",
    "             'average_review_score',\n",
    "             'category1_encoded',\n",
    "             'category2_encoded',\n",
    "             'category3_encoded',\n",
    "             'rolling_mean_purchase',\n",
    "             'rolling_std_purchase',\n",
    "             'week_num',\n",
    "             'month',\n",
    "             'month_sin',\n",
    "             'month_cos',\n",
    "             'week_sin',\n",
    "             'week_cos']\n",
    "\n",
    "target_column = 'week_purchase_cnt'\n",
    "max_encoder_length = 24\n",
    "max_prediction_length = 1\n",
    "batch_size = 64\n",
    "\n",
    "# 모델 인스턴스 생성 및 학습\n",
    "tft_model = TFTModel(data_path, features, target_column, \n",
    "                    max_encoder_length, max_prediction_length, batch_size)\n",
    "tft_model.fit()\n",
    "\n",
    "# # 평가 및 예측\n",
    "# predictions = tft_model.evaluate()\n",
    "\n",
    "# # 예시 상품들에 대한 예측 시각화\n",
    "# sample_products = tft_model.df['product_id'].unique()[:3]\n",
    "# print(sample_products)\n",
    "# tft_model.plot_predictions(sample_products)\n",
    "\n",
    "# # 특정 상품의 다음 주 예측\n",
    "# tft_model.predict_next_week(sample_products[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3f54cd-1b40-41f1-8f99-bde10a700cc5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29849863-635a-4d29-af08-f5f81b5793b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Unique product_ids in training:\n",
    "['81182199627' '82224112824' '7781174622' '81432925718' '83439829432'\n",
    " '82855381541' '11659574428' '84366233772' '7506929597' '84759655602'\n",
    " '83715863755' '12261313683' '81432917824' '82256221887' '86168008312'\n",
    " '85946192170' '80036527278' '83018697152' '82748436013' '9061388274'\n",
    " '82035499213' '85747878342' '13381025014' '81769963134' '83586891935'\n",
    " '83496016535' '82518325576' '85219719569' '86074262188' '81883717746'\n",
    " '80150432791' '83196162009' '86543062469' '12549294732' '82512029192'\n",
    " '82343766518' '6766327742' '82245440734' '84338752005' '86520272366'\n",
    " '82697228937' '85635072114' '84266140607' '81592586014' '86564530041'\n",
    " '82473335098' '84630569977' '82155387209' '85556153151' '84417974537'\n",
    " '86011600522' '82987858627' '10990197556' '86552777853' '81490308013'\n",
    " '82374636770' '82518358819' '86316244276' '84938641455' '83396511185'\n",
    " '83452559067' '11985210471' '82442958695' '85113603549' '81743617172'\n",
    " '86596962625' '83989019167' '83058672505' '80151600598' '85489125061'\n",
    " '86744054774' '86716255500' '86734323610' '82116389223' '82094228515'\n",
    " '9694820143' '86575042896' '86001457602' '13147435734' '10194331314'\n",
    " '10129739316' '11831091502' '81162882669' '9879202686' '80179015780'\n",
    " '80437478266' '80585484638' '81011449761' '81022718754' '81376726447'\n",
    " '81307567948' '11545229477' '12099494038' '80998837833' '85243095782'\n",
    " '82431335523' '83032577198' '84482901377' '83722394844' '84575983577'\n",
    " '84590609622' '83060973890' '85577317693' '83124441105' '84651414760'\n",
    " '83148914744' '82224200414' '8866795532' '10583217322' '82375566008'\n",
    " '83916701408' '82629272072' '83969946618' '82803508510' '82554079402'\n",
    " '83835508396' '82493840170' '8747453599' '84253456764' '85622876945'\n",
    " '82909794326' '86196224969' '82493839762' '86380103731' '86253563442'\n",
    " '82109132933' '86718087741' '84920211651' '83355904195' '85338623974'\n",
    " '86768411788' '86766394229' '85022115898' '83342948311' '82194575327'\n",
    " '85080929940' '84878502957' '83450413204' '85141197525' '86677200634'\n",
    " '81898265092' '86003052939' '82167582057' '86779626334' '83469823485'\n",
    " '82161091686' '82154885060' '84878420163' '85903271022' '84852924321'\n",
    " '83353405499' '83729421264' '81165039187' '83817222587' '83844563428'\n",
    " '85676427375' '85827942083' '83886328887' '80137720714' '80070124325'\n",
    " '83459206628' '80538088436' '83595717401' '81076354187' '81072587300'\n",
    " '85750841326' '81044807932' '83588503201' '81037586677' '81029307872'\n",
    " '83588420791' '85786543730' '83582241229' '83565897543' '80916609335'\n",
    " '80909194289' '80771696270' '83544700181' '80471449409' '80466364794'\n",
    " '80447378951' '80260009881' '85765526144' '83892959540' '85465017384'\n",
    " '84861658923' '84773371122' '11779789592' '84739735201' '11913737888'\n",
    " '11976178121' '84684925054' '85394939977' '10651348891' '10790326479'\n",
    " '10839131148' '11076937403' '11199076229' '85008754353' '11218721969'\n",
    " '11421983102' '11598092065' '85050087425' '11239882600' '84595711640'\n",
    " '84240189171' '84223183690' '84195356355' '84051723307' '13399566549'\n",
    " '80027880899' '83988837837' '83956059687' '83945044942' '85637826843'\n",
    " '83944786359' '83937248946' '85603598515' '84593454966' '85578911636'\n",
    " '84486897225' '85602479174' '12402425757' '12406570359' '84371446205'\n",
    " '84345558721' '12664919543' '12823927460' '83442851433' '82261431626'\n",
    " '82250264065' '82244034272' '82243364662' '82271542438' '82239755179'\n",
    " '82235144540' '82213682976' '86634985389' '82203873495' '82201860750'\n",
    " '82198829607' '82237266787' '82278485824' '82280680602' '82413203779'\n",
    " '82409708164' '82396216124' '86512643840' '82386649071' '82371516080'\n",
    " '82355482914' '82308116319' '82298266676' '82293068454' '82289916004'\n",
    " '82288244906' '82196153989' '82182472219' '81944633724' '86750799549'\n",
    " '81251971550' '81817765030' '81743662525' '81683115256' '81500447696'\n",
    " '81950814342' '82423604772' '81977652281' '82177741646' '86695180428'\n",
    " '82141150395' '82114593556' '82100081230' '82084083327' '82075021040'\n",
    " '82057321861' '82048947651' '83441947093' '86512422117' '82439360120'\n",
    " '83165781839' '83154350949' '83148737984' '83145373731' '86015286906'\n",
    " '83168990735' '83046474193' '83027433275' '83026221734' '86080579187'\n",
    " '83176583239' '83185458721' '83423045215' '83412461028' '83405521509'\n",
    " '83379992650' '83357689027' '85994962324' '83339174174' '83265103095'\n",
    " '83242599500' '83227652369' '83195131633' '82978885242' '82964195824'\n",
    " '82938287668' '82584169242' '86307626335' '82579351820' '82506716986'\n",
    " '82498044964' '86361182801' '82492189459' '82484077890' '82450880093'\n",
    " '82612392967' '86258398286' '82644613191' '82923956263' '82920847039'\n",
    " '82904703004' '82888223999' '86238161590' '82839137204' '82797428689'\n",
    " '86238199436' '82731789250' '82725368080' '82663901180' '82658175350'\n",
    " '10316027559' '6356018199' '9645322788' '10095244123' '8808295228'\n",
    " '10022619427' '9508439307' '10276314469' '8597689401' '8974847230'\n",
    " '8678210047' '10182440260' '8218195764' '6617143657' '6485255842'\n",
    " '9555749627' '86865818817' '86892789310' '86340219528' '82617930980'\n",
    " '86947868795' '86615650299' '11195202991' '86931391148' '86985198344'\n",
    " '87034617231' '87002755797' '85674849480' '87077078211' '87134721472'\n",
    " '13154622884' '87154257622' '87156542226' '87171220291' '87207387394'\n",
    " '87044616207' '83021652171' '87194315468' '87256966784' '87297137510'\n",
    " '87300027307' '87354688430' '87416266422' '87434901086' '87233647417'\n",
    " '84739899624' '86984516623' '87477209865' '82366359065' '10857366633'\n",
    " '82330465832' '87612495327' '87631394749' '87616961155' '87642859084'\n",
    " '81999320307' '9270155631' '87604505038' '85829941612' '87647012257'\n",
    " '87603328228' '87528396162' '87664970794' '87703272849' '87730178392'\n",
    " '83890388520' '86644487746' '87737396955' '87730829177' '87958278370'\n",
    " '87937248809' '87965476173' '87149885404' '87844396267' '87962457602'\n",
    " '87952389253' '87921020002' '88012479244' '87991745133' '87896780670'\n",
    " '87817018176' '87988876731' '88038712401' '88081241709' '82562973333'\n",
    " '88098187751' '88113889505' '88042110947' '88042074458' '87226364197'\n",
    " '88117899204' '88119947030' '88218200275' '88100622609' '88208430684']\n",
    "\n",
    "Unique product_ids in validation:\n",
    "['82439360120' '82100081230' '82562973333' '83227652369' '86307626335'\n",
    " '87730829177' '84938641455' '83412461028' '85022115898' '80771696270'\n",
    " '86677200634' '13147435734' '6356018199' '83423045215' '82923956263'\n",
    " '85008754353' '81376726447' '83242599500' '82167582057' '82484077890'\n",
    " '12823927460' '85338623974' '80916609335' '85827942083' '87603328228'\n",
    " '81432917824' '80036527278' '87965476173' '82109132933' '83439829432'\n",
    " '82518358819' '82473335098' '12664919543' '82177741646' '87154257622'\n",
    " '86984516623' '83441947093' '6766327742' '82386649071' '82584169242'\n",
    " '83937248946' '84482901377' '82239755179' '87233647417' '10990197556'\n",
    " '8597689401' '7781174622' '83027433275' '83944786359' '82035499213'\n",
    " '82644613191' '82293068454' '84417974537' '81883717746' '86238161590'\n",
    " '86543062469' '88119947030' '83145373731' '80466364794' '85635072114'\n",
    " '84371446205' '83945044942' '82658175350' '85578911636' '82289916004'\n",
    " '86575042896' '11545229477' '82243364662' '83032577198' '87958278370'\n",
    " '86931391148' '83956059687' '82298266676' '11598092065' '87226364197'\n",
    " '87844396267' '86520272366' '10839131148' '86985198344' '83890388520'\n",
    " '82629272072' '86750799549' '82748436013' '84575983577' '81769963134'\n",
    " '83148914744' '83892959540' '82235144540' '10857366633' '82330465832'\n",
    " '83026221734' '11659574428' '82048947651' '80179015780' '83588420791'\n",
    " '85637826843' '82308116319' '82237266787' '8678210047' '81817765030'\n",
    " '86892789310' '84486897225' '83916701408' '83148737984' '85577317693'\n",
    " '84590609622' '80260009881' '87616961155' '84051723307' '85602479174'\n",
    " '11239882600' '83060973890' '11195202991' '84195356355' '82250264065'\n",
    " '82725368080' '81950814342' '82271542438' '87703272849' '84266140607'\n",
    " '81029307872' '11218721969' '86734323610' '86564530041' '11199076229'\n",
    " '83046474193' '82261431626' '85603598515' '84223183690' '83058672505'\n",
    " '84253456764' '82697228937' '87297137510' '88038712401' '81977652281'\n",
    " '80437478266' '82256221887' '84240189171' '9694820143' '81898265092'\n",
    " '86015286906' '86238199436' '83124441105' '87952389253' '82731789250'\n",
    " '83969946618' '8866795532' '82288244906' '84366233772' '86011600522'\n",
    " '11421983102' '82244034272' '83988837837' '81037586677' '87300027307'\n",
    " '82280680602' '82278485824' '80447378951' '81999320307' '87256966784'\n",
    " '84345558721' '82663901180' '86744054774' '11076937403' '85622876945'\n",
    " '81022718754' '83989019167' '82245440734' '86552777853' '9555749627'\n",
    " '84338752005' '81944633724' '88184129427' '86596962625' '87354688430'\n",
    " '83342948311' '9879202686' '86168008312' '82075021040' '82375566008'\n",
    " '87962457602' '83595717401' '82371516080' '82224112824' '87817018176'\n",
    " '87434901086' '87937248809' '86615650299' '81072587300' '84595711640'\n",
    " '83817222587' '87034617231' '85556153151' '84651414760' '85489125061'\n",
    " '11831091502' '11985210471' '88242709418' '81683115256' '82797428689'\n",
    " '81076354187' '85676427375' '11976178121' '82374636770' '83018697152'\n",
    " '82213682976' '83168990735' '87194315468' '8808295228' '88113889505'\n",
    " '81743617172' '10651348891' '88100622609' '82612392967' '9061388274'\n",
    " '83729421264' '81011449761' '83715863755' '86512643840' '84630569977'\n",
    " '83886328887' '10583217322' '82617930980' '11913737888' '86718087741'\n",
    " '84684925054' '83165781839' '83835508396' '81743662525' '83154350949'\n",
    " '82203873495' '82355482914' '80471449409' '11779789592' '84593454966'\n",
    " '87207387394' '86003052939' '87647012257' '82987858627' '8218195764'\n",
    " '88042074458' '8974847230' '86253563442' '86766394229' '82084083327'\n",
    " '10790326479' '81044807932' '85674849480' '82343766518' '82057321861'\n",
    " '83021652171' '80151600598' '83844563428' '83722394844' '82803508510'\n",
    " '85747878342' '86074262188' '7506929597' '87416266422' '80150432791'\n",
    " '82366359065' '83588503201' '84739735201' '82224200414' '87896780670'\n",
    " '81182199627' '80998837833' '86947868795' '82904703004' '86001457602'\n",
    " '83176583239' '82978885242' '80909194289' '80538088436' '80137720714'\n",
    " '82518325576' '87077078211' '83339174174' '87002755797' '82909794326'\n",
    " '85946192170' '87737396955' '9508439307' '80585484638' '88098187751'\n",
    " '87642859084' '86865818817' '83196162009' '80070124325' '82839137204'\n",
    " '81162882669' '85994962324' '82855381541' '82554079402' '82938287668'\n",
    " '88081241709' '82964195824' '83265103095' '86258398286' '83185458721'\n",
    " '81165039187' '82920847039' '80027880899' '86196224969' '83195131633'\n",
    " '86080579187' '82579351820' '88042110947' '87044616207' '82888223999'\n",
    " '82409708164' '87991745133' '84920211651' '10182440260' '82182472219'\n",
    " '84878502957' '86716255500' '86644487746' '85394939977' '88117899204'\n",
    " '87528396162' '82194575327' '84878420163' '10194331314' '84861658923'\n",
    " '6617143657' '86695180428' '85219719569' '10022619427' '82141150395'\n",
    " '85141197525' '82154885060' '85113603549' '82116389223' '87612495327'\n",
    " '87604505038' '82155387209' '10095244123' '85080929940' '87664970794'\n",
    " '82161091686' '85050087425' '82114593556' '10129739316' '85243095782'\n",
    " '82094228515' '86634985389' '87988876731' '88012479244' '84852924321'\n",
    " '82196153989' '84773371122' '85465017384' '87477209865' '10276314469'\n",
    " '82198829607' '84759655602' '10316027559' '84739899624' '82201860750'\n",
    " '12549294732' '87171220291' '86512422117' '86768411788' '87149885404'\n",
    " '82493839762' '83405521509' '86779626334' '83396511185' '87921020002'\n",
    " '82493840170' '87134721472' '83469823485' '6485255842' '81307567948'\n",
    " '87156542226' '83586891935' '86380103731' '83496016535' '12402425757'\n",
    " '83544700181' '82423604772' '81500447696' '82413203779' '83565897543'\n",
    " '87730178392' '85750841326' '86340219528' '81592586014' '83582241229'\n",
    " '85829941612' '82492189459' '81490308013' '82396216124' '85765526144'\n",
    " '82431335523' '12099494038' '12261313683' '82498044964' '9645322788'\n",
    " '81251971550' '87631394749' '83355904195' '88218200275' '13399566549'\n",
    " '88206700121' '81432925718' '82506716986' '83353405499' '86361182801'\n",
    " '83450413204' '85786543730' '82512029192' '82442958695' '13381025014'\n",
    " '83452559067' '83379992650' '13154622884' '85903271022' '9270155631'\n",
    " '8747453599' '83459206628' '12406570359' '88208430684' '82450880093'\n",
    " '83442851433' '83357689027' '86316244276' '88222671639' '88291977922'\n",
    " '87896784213' '88284816214' '88351942304' '88303490649' '88321577019'\n",
    " '88347463796' '88283419814' '88334988204' '88380500239' '88401777165'\n",
    " '88388649996' '88388890335' '88415339810' '88415318369' '88388533454'\n",
    " '88356536951' '88411622436' '88438439266' '88378913358' '88415191727'\n",
    " '88453896846' '88473568229' '88459191933']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
